{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NNEVE is a collection of neural network based solutions to physics based problems. As for now only network for quantum oscillator approximation is fully implemented. Hopefully soon will arrive neural network for solving Navier-Stokes equation based on limited number of measurement points.. Installation # This project is uploaded to PyPI as nneve , therefore can be installed with pip install ( Python 3.7 or newer is required) pip install nneve Collecting nneve Downloading nneve-22.6.28-py3-none-any.whl (20 kB) Successfully installed nneve-22.6.28 Quick example # To view quantum oscillator approximation for states 1 to 7 you can load precalculated weights and acquire model object with following snippet: 1 2 3 4 5 6 7 8 9 10 from matplotlib import pyplot as plt from nneve.quantum_oscillator.examples import default_qo_network # acquire network object with precalculated weights # for quantum oscillator state 1 (base) network = default_qo_network ( state = 1 ) network . plot_solution () plt . plot () To manually run learning cycle check out \"How to run QONetwork learning cycle\" in Quantum Oscillator section of docs. Documentation # Online documentation is available at argmaster.github.io/NNEVE/ Builing docs locally is possible and well automated using tox virtual environments. To be able to build documentation you have to acquire Python==3.8 and tox>=3.24 , then you will be able to build docs tox -e docs docs create: ~/repos/nneve/.tox/docs docs installdeps: -rrequirements-docs.txt ... docs run-test: commands[0] | mkdocs build INFO - [macros] - Macros arguments: ... INFO - Documentation built in 0.49 seconds docs: commands succeeded congratulations :)","title":"Home"},{"location":"#installation","text":"This project is uploaded to PyPI as nneve , therefore can be installed with pip install ( Python 3.7 or newer is required) pip install nneve Collecting nneve Downloading nneve-22.6.28-py3-none-any.whl (20 kB) Successfully installed nneve-22.6.28","title":"Installation"},{"location":"#quick-example","text":"To view quantum oscillator approximation for states 1 to 7 you can load precalculated weights and acquire model object with following snippet: 1 2 3 4 5 6 7 8 9 10 from matplotlib import pyplot as plt from nneve.quantum_oscillator.examples import default_qo_network # acquire network object with precalculated weights # for quantum oscillator state 1 (base) network = default_qo_network ( state = 1 ) network . plot_solution () plt . plot () To manually run learning cycle check out \"How to run QONetwork learning cycle\" in Quantum Oscillator section of docs.","title":"Quick example"},{"location":"#documentation","text":"Online documentation is available at argmaster.github.io/NNEVE/ Builing docs locally is possible and well automated using tox virtual environments. To be able to build documentation you have to acquire Python==3.8 and tox>=3.24 , then you will be able to build docs tox -e docs docs create: ~/repos/nneve/.tox/docs docs installdeps: -rrequirements-docs.txt ... docs run-test: commands[0] | mkdocs build INFO - [macros] - Macros arguments: ... INFO - Documentation built in 0.49 seconds docs: commands succeeded congratulations :)","title":"Documentation"},{"location":"changelog/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Calendar Versioning . [22.8.20] - 2022-08-20 # Fix examples in README and docs index Add docs for QONetwork Prepare documentation for quantum_oscillator sub-package. [22.6.27] - 2022-06-27 # Implement neural network for Quantum Oscillator approximation","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Calendar Versioning .","title":"Changelog"},{"location":"changelog/#22820-2022-08-20","text":"Fix examples in README and docs index Add docs for QONetwork Prepare documentation for quantum_oscillator sub-package.","title":"[22.8.20] - 2022-08-20"},{"location":"changelog/#22627-2022-06-27","text":"Implement neural network for Quantum Oscillator approximation","title":"[22.6.27] - 2022-06-27"},{"location":"license/","text":"GNU LESSER GENERAL PUBLIC LICENSE # Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. This version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below. 0. Additional Definitions. # As used herein, \"this License\" refers to version 3 of the GNU Lesser General Public License, and the \"GNU GPL\" refers to version 3 of the GNU General Public License. \"The Library\" refers to a covered work governed by this License, other than an Application or a Combined Work as defined below. An \"Application\" is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library. A \"Combined Work\" is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the \"Linked Version\". The \"Minimal Corresponding Source\" for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version. The \"Corresponding Application Code\" for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work. 1. Exception to Section 3 of the GNU GPL. # You may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL. 2. Conveying Modified Versions. # If you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version: a) under this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or b) under the GNU GPL, with none of the additional permissions of this License applicable to that copy. 3. Object Code Incorporating Material from Library Header Files. # The object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following: a) Give prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the object code with a copy of the GNU GPL and this license document. 4. Combined Works. # You may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following: a) Give prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the Combined Work with a copy of the GNU GPL and this license document. c) For a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document. d) Do one of the following: - 0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source. - 1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version. e) Provide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.) 5. Combined Libraries. # You may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License. b) Give prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work. 6. Revised Versions of the GNU Lesser General Public License. # The Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation. If the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy's public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.","title":"License"},{"location":"license/#gnu-lesser-general-public-license","text":"Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. This version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below.","title":"GNU LESSER GENERAL PUBLIC LICENSE"},{"location":"license/#0-additional-definitions","text":"As used herein, \"this License\" refers to version 3 of the GNU Lesser General Public License, and the \"GNU GPL\" refers to version 3 of the GNU General Public License. \"The Library\" refers to a covered work governed by this License, other than an Application or a Combined Work as defined below. An \"Application\" is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library. A \"Combined Work\" is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the \"Linked Version\". The \"Minimal Corresponding Source\" for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version. The \"Corresponding Application Code\" for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work.","title":"0. Additional Definitions."},{"location":"license/#1-exception-to-section-3-of-the-gnu-gpl","text":"You may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL.","title":"1. Exception to Section 3 of the GNU GPL."},{"location":"license/#2-conveying-modified-versions","text":"If you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version: a) under this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or b) under the GNU GPL, with none of the additional permissions of this License applicable to that copy.","title":"2. Conveying Modified Versions."},{"location":"license/#3-object-code-incorporating-material-from-library-header-files","text":"The object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following: a) Give prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the object code with a copy of the GNU GPL and this license document.","title":"3. Object Code Incorporating Material from Library Header Files."},{"location":"license/#4-combined-works","text":"You may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following: a) Give prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the Combined Work with a copy of the GNU GPL and this license document. c) For a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document. d) Do one of the following: - 0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source. - 1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version. e) Provide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.)","title":"4. Combined Works."},{"location":"license/#5-combined-libraries","text":"You may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License. b) Give prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.","title":"5. Combined Libraries."},{"location":"license/#6-revised-versions-of-the-gnu-lesser-general-public-license","text":"The Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation. If the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy's public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.","title":"6. Revised Versions of the GNU Lesser General Public License."},{"location":"develop/docs/","text":"Documentation # Handy links # Markdownguide.org Basic Syntax Markdownguide.org Extended Syntax MkDocs-Material Reference MkDocs # MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introductory tutorial, then check the User Guide for more information. Main webpage User guide We are also using Material for MkDocs theme for documentation which is a separate package. Main webpage Reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Usage: mkdocs [OPTIONS] COMMAND [ARGS]... MkDocs - Project documentation with Markdown. Options: -V, --version Show the version and exit. -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit. Commands: build Build the MkDocs documentation gh-deploy Deploy your documentation to GitHub Pages new Create a new MkDocs project serve Run the builtin development server Live server # Runs development server, which automatically mirrors changes in source code. Development server is by default available at http://127.0.0.1:8000/ 1 mkdocs serve Full CLI help: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Usage: mkdocs serve [OPTIONS] Run the builtin development server Options: -a, --dev-addr <IP:PORT> IP address and port to serve documentation locally (default: localhost:8000) --livereload Enable the live reloading in the development server (this is the default) --no-livereload Disable the live reloading in the development server. --dirtyreload Enable the live reloading in the development server, but only re-build files that have changed --watch-theme Include the theme in list of files to watch for live reloading. Ignored when live reload is not used. -f, --config-file FILENAME Provide a specific MkDocs config -s, --strict Enable strict mode. This will cause MkDocs to abort the build on any warnings. -t, --theme [material|mkdocs|readthedocs] The theme to use when building your documentation. --use-directory-urls / --no-directory-urls Use directory URLs when building pages (the default). -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit. Build documentation # Builds documentation, all generated files are by default saved to site/ folder. 1 mkdocs build Full CLI help: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Usage: mkdocs build [OPTIONS] Build the MkDocs documentation Options: -c, --clean / --dirty Remove old files from the site_dir before building (the default). -f, --config-file FILENAME Provide a specific MkDocs config -s, --strict Enable strict mode. This will cause MkDocs to abort the build on any warnings. -t, --theme [mkdocs|material|readthedocs] The theme to use when building your documentation. --use-directory-urls / --no-directory-urls Use directory URLs when building pages (the default). -d, --site-dir PATH The directory to output the result of the documentation build. -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit.","title":"Documentation"},{"location":"develop/docs/#documentation","text":"","title":"Documentation"},{"location":"develop/docs/#handy-links","text":"Markdownguide.org Basic Syntax Markdownguide.org Extended Syntax MkDocs-Material Reference","title":"Handy links"},{"location":"develop/docs/#mkdocs","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introductory tutorial, then check the User Guide for more information. Main webpage User guide We are also using Material for MkDocs theme for documentation which is a separate package. Main webpage Reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Usage: mkdocs [OPTIONS] COMMAND [ARGS]... MkDocs - Project documentation with Markdown. Options: -V, --version Show the version and exit. -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit. Commands: build Build the MkDocs documentation gh-deploy Deploy your documentation to GitHub Pages new Create a new MkDocs project serve Run the builtin development server","title":"MkDocs"},{"location":"develop/docs/#live-server","text":"Runs development server, which automatically mirrors changes in source code. Development server is by default available at http://127.0.0.1:8000/ 1 mkdocs serve Full CLI help: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Usage: mkdocs serve [OPTIONS] Run the builtin development server Options: -a, --dev-addr <IP:PORT> IP address and port to serve documentation locally (default: localhost:8000) --livereload Enable the live reloading in the development server (this is the default) --no-livereload Disable the live reloading in the development server. --dirtyreload Enable the live reloading in the development server, but only re-build files that have changed --watch-theme Include the theme in list of files to watch for live reloading. Ignored when live reload is not used. -f, --config-file FILENAME Provide a specific MkDocs config -s, --strict Enable strict mode. This will cause MkDocs to abort the build on any warnings. -t, --theme [material|mkdocs|readthedocs] The theme to use when building your documentation. --use-directory-urls / --no-directory-urls Use directory URLs when building pages (the default). -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit.","title":"Live server"},{"location":"develop/docs/#build-documentation","text":"Builds documentation, all generated files are by default saved to site/ folder. 1 mkdocs build Full CLI help: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Usage: mkdocs build [OPTIONS] Build the MkDocs documentation Options: -c, --clean / --dirty Remove old files from the site_dir before building (the default). -f, --config-file FILENAME Provide a specific MkDocs config -s, --strict Enable strict mode. This will cause MkDocs to abort the build on any warnings. -t, --theme [mkdocs|material|readthedocs] The theme to use when building your documentation. --use-directory-urls / --no-directory-urls Use directory URLs when building pages (the default). -d, --site-dir PATH The directory to output the result of the documentation build. -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit.","title":"Build documentation"},{"location":"develop/feature/","text":"Feature flow # This project uses GitHub flow as it's main workflow model. Simplified visualization can be seen on graph below: %%{init: { 'theme': 'forest' , 'themeVariables': { 'git0': '#4db85f', 'git1': '#49b391', 'git2': '#59a7ff', 'git3': '#d93261', 'git4': '#00ffff', 'git5': '#ffff00', 'git6': '#ff00ff', 'git7': '#00ffff' } } }%% gitGraph commit tag:\"0.0.0\" branch feature checkout feature branch private checkout private commit commit checkout feature merge private branch private2 checkout private2 commit commit checkout feature merge private2 commit tag:\"1.0.0\" checkout main merge feature Clone repository # Hint You can skip this step if you already have a clone 1 git clone https://github.com/Argmaster/nneve.git Checking out main branch # Make sure we are on main branch. 1 git checkout main Pull changes from origin # Hint You can skip this step if you just cloned the repository 1 git pull --ff Create feature branch # Create new branch for our feature called (replace with whatever you want) feature_name . feature/ prefix is required because of convention. Learn about branches 1 git checkout -b feature/feature_name Check repository status # 1 git status Result should be similar to this: 1 2 On branch feature/feature_name nothing to commit, working tree clean Commit-test-push cycle # Your work on a feature should be divided into many steps during which you will add new units to the system. Each unit should have a set of tests to verify its operation. Formatting & Quality checks # Run code quality checks with tox to quickly fix most obvious issues in your code. 1 tox -e check 1 tox -e check Run test suite for Python interpreter versions you have locally # Run test suites on available interpreters with 1 tox -e py37 If the tests fail, you have to repeat steps 1 and 2. Omission of the corrections will result in your changes being rejected by the CI tests executed for the pull request. Add all changes to staging area with # 1 git add * You can list file paths instead of using the asterisk symbol if you know you can add many unwanted files. If these unwanted files regularly appear in the codebase, add them to the .gitignore file. Check staging area # 1 git status If any files staged for commit shouldn't be there, unstage them with 1 git restore --staged <file> Commit changes # Now use commit command to send changes to git history 1 git commit This command will open text editor for you, waiting for commit description. You can use 1 git commit -m \"commit message\" to add commit title and omit long description. The commit title should not be longer than 50 characters. How to write a Git Commit Message Good Commit Messages: A Practical Git Guide Push changes to remote branch # 1 git push -u origin feature/feature_name For each subsequent push from this branch, you can omit -u origin feature/feature_name 1 git push Create pull request # Visit pull requests and create PR for you feature. Read in GitHub docs about pull requests. Request review & wait for CI checks # Now you can request a pull request review, as it's described here . Before your changes can be merged into another branch, at least one person should see them, and share their thoughts about them with you. If you are prompted to make corrections, do so immediately and do not apply your changes without fixes. Go back to Commit-test-push . You changes should also pass all tests ran by CI system ( Github Actions ). If the tests fail, corrections will also be required before continuing. Merge PR # After receiving a positive response from the reviewer and passing the tests, the pull request can be merged. About merge conflicts About pull request merges .","title":"Feature flow"},{"location":"develop/feature/#feature-flow","text":"This project uses GitHub flow as it's main workflow model. Simplified visualization can be seen on graph below: %%{init: { 'theme': 'forest' , 'themeVariables': { 'git0': '#4db85f', 'git1': '#49b391', 'git2': '#59a7ff', 'git3': '#d93261', 'git4': '#00ffff', 'git5': '#ffff00', 'git6': '#ff00ff', 'git7': '#00ffff' } } }%% gitGraph commit tag:\"0.0.0\" branch feature checkout feature branch private checkout private commit commit checkout feature merge private branch private2 checkout private2 commit commit checkout feature merge private2 commit tag:\"1.0.0\" checkout main merge feature","title":"Feature flow"},{"location":"develop/feature/#clone-repository","text":"Hint You can skip this step if you already have a clone 1 git clone https://github.com/Argmaster/nneve.git","title":"Clone repository"},{"location":"develop/feature/#checking-out-main-branch","text":"Make sure we are on main branch. 1 git checkout main","title":"Checking out main branch"},{"location":"develop/feature/#pull-changes-from-origin","text":"Hint You can skip this step if you just cloned the repository 1 git pull --ff","title":"Pull changes from origin"},{"location":"develop/feature/#create-feature-branch","text":"Create new branch for our feature called (replace with whatever you want) feature_name . feature/ prefix is required because of convention. Learn about branches 1 git checkout -b feature/feature_name","title":"Create feature branch"},{"location":"develop/feature/#check-repository-status","text":"1 git status Result should be similar to this: 1 2 On branch feature/feature_name nothing to commit, working tree clean","title":"Check repository status"},{"location":"develop/feature/#commit-test-push-cycle","text":"Your work on a feature should be divided into many steps during which you will add new units to the system. Each unit should have a set of tests to verify its operation.","title":"Commit-test-push cycle"},{"location":"develop/feature/#formatting-quality-checks","text":"Run code quality checks with tox to quickly fix most obvious issues in your code. 1 tox -e check 1 tox -e check","title":"Formatting &amp; Quality checks"},{"location":"develop/feature/#run-test-suite-for-python-interpreter-versions-you-have-locally","text":"Run test suites on available interpreters with 1 tox -e py37 If the tests fail, you have to repeat steps 1 and 2. Omission of the corrections will result in your changes being rejected by the CI tests executed for the pull request.","title":"Run test suite for Python interpreter versions you have locally"},{"location":"develop/feature/#add-all-changes-to-staging-area-with","text":"1 git add * You can list file paths instead of using the asterisk symbol if you know you can add many unwanted files. If these unwanted files regularly appear in the codebase, add them to the .gitignore file.","title":"Add all changes to staging area with"},{"location":"develop/feature/#check-staging-area","text":"1 git status If any files staged for commit shouldn't be there, unstage them with 1 git restore --staged <file>","title":"Check staging area"},{"location":"develop/feature/#commit-changes","text":"Now use commit command to send changes to git history 1 git commit This command will open text editor for you, waiting for commit description. You can use 1 git commit -m \"commit message\" to add commit title and omit long description. The commit title should not be longer than 50 characters. How to write a Git Commit Message Good Commit Messages: A Practical Git Guide","title":"Commit changes"},{"location":"develop/feature/#push-changes-to-remote-branch","text":"1 git push -u origin feature/feature_name For each subsequent push from this branch, you can omit -u origin feature/feature_name 1 git push","title":"Push changes to remote branch"},{"location":"develop/feature/#create-pull-request","text":"Visit pull requests and create PR for you feature. Read in GitHub docs about pull requests.","title":"Create pull request"},{"location":"develop/feature/#request-review-wait-for-ci-checks","text":"Now you can request a pull request review, as it's described here . Before your changes can be merged into another branch, at least one person should see them, and share their thoughts about them with you. If you are prompted to make corrections, do so immediately and do not apply your changes without fixes. Go back to Commit-test-push . You changes should also pass all tests ran by CI system ( Github Actions ). If the tests fail, corrections will also be required before continuing.","title":"Request review &amp; wait for CI checks"},{"location":"develop/feature/#merge-pr","text":"After receiving a positive response from the reviewer and passing the tests, the pull request can be merged. About merge conflicts About pull request merges .","title":"Merge PR"},{"location":"develop/formatting/","text":"Code formatting # black # black is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, black gives you speed, determinism, and freedom from pycodestyle nagging about formatting. You will save time and mental energy for more important matters. You can view black configuration in pyproject.toml file, in [tool.black] section. Manual usage valid for this project: 1 black . isort # isort your imports, so you don't have to. isort is a Python utility / library to sort imports alphabetically, and automatically separated into sections and by type. You can view isort configuration in .isort.cfg file. Manual usage valid for this project: 1 isort . docformatter # docformatter currently automatically formats docstrings to follow a subset of the PEP 257 conventions. Below are the relevant items quoted from PEP 257. For consistency, always use triple double quotes around docstrings. Triple quotes are used even though the string fits on one line. Multi-line docstrings consist of a summary line just like a one-line docstring, followed by a blank line, followed by a more elaborate description. Unless the entire docstring fits on a line, place the closing quotes on a line by themselves. docformatter also handles some of the PEP 8 conventions. Don\u2019t write string literals that rely on significant trailing whitespace. Such trailing whitespace is visually indistinguishable and some editors (or more recently, reindent.py) will trim them. Manual usage valid for this project: 1 docformatter -r source/ scripts/ --in-place --docstring-length 75 75 -e .tox,.eggs,build,dist,typings,.temp","title":"Formatting code"},{"location":"develop/formatting/#code-formatting","text":"","title":"Code formatting"},{"location":"develop/formatting/#black","text":"black is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, black gives you speed, determinism, and freedom from pycodestyle nagging about formatting. You will save time and mental energy for more important matters. You can view black configuration in pyproject.toml file, in [tool.black] section. Manual usage valid for this project: 1 black .","title":"black"},{"location":"develop/formatting/#isort","text":"isort your imports, so you don't have to. isort is a Python utility / library to sort imports alphabetically, and automatically separated into sections and by type. You can view isort configuration in .isort.cfg file. Manual usage valid for this project: 1 isort .","title":"isort"},{"location":"develop/formatting/#docformatter","text":"docformatter currently automatically formats docstrings to follow a subset of the PEP 257 conventions. Below are the relevant items quoted from PEP 257. For consistency, always use triple double quotes around docstrings. Triple quotes are used even though the string fits on one line. Multi-line docstrings consist of a summary line just like a one-line docstring, followed by a blank line, followed by a more elaborate description. Unless the entire docstring fits on a line, place the closing quotes on a line by themselves. docformatter also handles some of the PEP 8 conventions. Don\u2019t write string literals that rely on significant trailing whitespace. Such trailing whitespace is visually indistinguishable and some editors (or more recently, reindent.py) will trim them. Manual usage valid for this project: 1 docformatter -r source/ scripts/ --in-place --docstring-length 75 75 -e .tox,.eggs,build,dist,typings,.temp","title":"docformatter"},{"location":"develop/quality_checks/","text":"Code quality checks # Running single test file To run single test file with pytest 1 pytest tests/test_folder/test_feature.py -rP You can select single test too 1 pytest tests/test_folder/test_feature.py -rP -k test_name flake8 # Flake8 is a wrapper around these tools: PyFlakes which checks Python source files for errors. pycodestyle , a tool to check your Python code against some of the style conventions in PEP 8. Ned Batchelder\u2019s McCabe script for checking McCabe complexity. See list of awesome flake8 plugins List of included 3rd-party plugins: flake8-alfred - warn on unsafe/obsolete symbols. flake8-alphabetize - checker for alphabetizing import and all . flake8-broken-line - forbid backslashes for line breaks. flake8-bugbear - finding likely bugs and design problems in your program. flake8-builtins - check for python builtins being used as variables or parameters. flake8-comprehensions - check for invalid list/set/dict comprehensions. flake8-docstrings - uses pydocstyle to check docstrings flake8-eradicate - find commented out (or so called \"dead\") code. flake8-functions - report on issues with functions. flake8-functions-names - validates function names, decomposition and conformity with annotations. Conventions from here and here . flake8-printf-formatting - forbids printf-style string formatting flake8-pytest-style - checking common style issues or inconsistencies with pytest-based tests. flake8-simplify - helps you simplify your code. pep8-naming - check your code against PEP 8 naming conventions. flake8-expression-complexity - validates expression complexity and stops you from creating monstrous multi-line expressions. flake8-cognitive-complexity - validates cognitive functions complexity. pre-commit # A framework for managing and maintaining multi-language pre-commit hooks. Git hook scripts are useful for identifying simple issues before submission to code review. We run our hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks. List of hooks # isort black flake8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 - flake8-alfred - flake8-alphabetize - flake8-broken-line - flake8-bugbear - flake8-builtins - flake8-comprehensions - flake8-docstrings - flake8-eradicate - flake8-functions - flake8-functions-names - flake8-printf-formatting - flake8-pytest-style - flake8-simplify - pep8-naming - flake8-cognitive-complexity - flake8-expression-complexity docformatter pre-commit-hooks 1 2 3 4 5 6 7 - trailing-whitespace - end-of-file-fixer - debug-statements - check-added-large-file - no-commit-to-branch - requirements-txt-fixer - trailing-whitespace","title":"Quality checks"},{"location":"develop/quality_checks/#code-quality-checks","text":"Running single test file To run single test file with pytest 1 pytest tests/test_folder/test_feature.py -rP You can select single test too 1 pytest tests/test_folder/test_feature.py -rP -k test_name","title":"Code quality checks"},{"location":"develop/quality_checks/#flake8","text":"Flake8 is a wrapper around these tools: PyFlakes which checks Python source files for errors. pycodestyle , a tool to check your Python code against some of the style conventions in PEP 8. Ned Batchelder\u2019s McCabe script for checking McCabe complexity. See list of awesome flake8 plugins List of included 3rd-party plugins: flake8-alfred - warn on unsafe/obsolete symbols. flake8-alphabetize - checker for alphabetizing import and all . flake8-broken-line - forbid backslashes for line breaks. flake8-bugbear - finding likely bugs and design problems in your program. flake8-builtins - check for python builtins being used as variables or parameters. flake8-comprehensions - check for invalid list/set/dict comprehensions. flake8-docstrings - uses pydocstyle to check docstrings flake8-eradicate - find commented out (or so called \"dead\") code. flake8-functions - report on issues with functions. flake8-functions-names - validates function names, decomposition and conformity with annotations. Conventions from here and here . flake8-printf-formatting - forbids printf-style string formatting flake8-pytest-style - checking common style issues or inconsistencies with pytest-based tests. flake8-simplify - helps you simplify your code. pep8-naming - check your code against PEP 8 naming conventions. flake8-expression-complexity - validates expression complexity and stops you from creating monstrous multi-line expressions. flake8-cognitive-complexity - validates cognitive functions complexity.","title":"flake8"},{"location":"develop/quality_checks/#pre-commit","text":"A framework for managing and maintaining multi-language pre-commit hooks. Git hook scripts are useful for identifying simple issues before submission to code review. We run our hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks.","title":"pre-commit"},{"location":"develop/quality_checks/#list-of-hooks","text":"isort black flake8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 - flake8-alfred - flake8-alphabetize - flake8-broken-line - flake8-bugbear - flake8-builtins - flake8-comprehensions - flake8-docstrings - flake8-eradicate - flake8-functions - flake8-functions-names - flake8-printf-formatting - flake8-pytest-style - flake8-simplify - pep8-naming - flake8-cognitive-complexity - flake8-expression-complexity docformatter pre-commit-hooks 1 2 3 4 5 6 7 - trailing-whitespace - end-of-file-fixer - debug-statements - check-added-large-file - no-commit-to-branch - requirements-txt-fixer - trailing-whitespace","title":"List of hooks"},{"location":"develop/tox_basics/","text":"Tox basics # A virtual environment is a Python environment such that the Python interpreter, libraries and scripts installed into it are isolated from those installed in other virtual environments, and (by default) any libraries installed in a \u201csystem\u201d Python, i.e., one which is installed as part of your operating system. What is tox? # tox is a generic virtualenv management and test command line tool you can use for: checking that your package installs correctly with different Python versions and interpreters running your tests in each of the environments, configuring your test tool of choice acting as a frontend to Continuous Integration servers, greatly reducing boilerplate and merging CI and shell-based testing. Handy Links # To read more about tox, visit it's documentation. tox global settings tox environments configuration tox substitutions Generating environments, conditional settings Environmental variables Full tox cli documentation tox examples Our perspective # Every complex Python project requires multiple tools for development and deployment. Those are mostly related to test suite running, checking quality of code, developing and building documentation and building distribution packages. Usually those are tedious tasks and they are at the top of the lists of tasks to automate. Here comes tox tox can be used as a reliable replacement for manually written scripts. It's designed to run predefined series of command in automatically generated Python virtual environment. It was designed for Python ecosystem and is widely used along Python projects. It is compatible with other Python tools out of the box. All the configuration is contained in tox.ini file and is completely static. Basic usage # To invoke single environment with tox you have to memorize one simple command: 1 tox -e envname Where envname is replaced 1:1 with name of any environments listed below. Info You can also use tox command without any arguments to run checks for all supported python versions. Be aware that it is really time consuming. List of all configured environments # Simplicity of creating tox managed environments allows us to create highly specialized environments with minimal boilerplate. devenv # Stands for development environment (important when using IDE like Visual Studio Code or PyCharm). When selecting interpreter for your IDE, devenv is a right one to pick. This environment is meant to contain all tools important for continuos development including linters, formatters, building tools, packaging tools and everything else listed in requirements-dev.txt It is really heavy and expensive to create because of complexity of installation. Every call of tox -e devenv will completely recreate the environment. Danger Running tox -e devenv completely reinstalls environment - it's time consuming. It is designed in such way many due to the fact that during development there is no need to recreate it until something brakes, and then it's handy to simplify reinstallation how much possible. Hint Running this environment will install pre-commit. To select Python from devenv as interpreter in Visual Studio Code, use Ctrl + Shift + P and type Python: Select Interpreter , then hit Enter , select Enter interpreter path , pick Find and navigate to python.exe in .tox/devenv/bin (unix) or .tox/devev/scripts (windows). This environment is rather bullet proof in comparison to other non-utility environments (mainly test runners). It should just install on demand, and every failure should be considered and fixed permanently. List of included dependencies: requirement.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 click> = 8.1.0,<8.2.0 matplotlib> = 3.5.0,<3.6.0 numpy> = 1.21.0,<1.22.0 packaging> = 21.3.0,<21.4.0 pandas> = 1.3.0,<1.4.0 rich> = 12.0.0,<12.1.0 scipy> = 1.7.0,<1.8.0 tensorflow> = 2.8.0,<2.9.0 pillow> = 9.1.0,<9.2.0 psutil> = 5.9.0,<5.10.0 py-cpuinfo> = 8.0.0,<8.1.0 pydantic> = 1.9.0,<1.10.0 backports.cached-property> = 1.0.2 requirement-test.txt 1 2 3 4 5 6 7 8 # Pytest + plugins pytest = =7.1.2 pytest-cov = =3.0.0 # Hypothesis https://hypothesis.readthedocs.io/en/latest/data.html hypothesis> = 6.46.0,<6.47.0 # Static typechecking mypy = =0.950 lxml = =4.8.0 requirement-dev.txt 1 2 3 4 5 -r requirements.txt -r requirements-check.txt -r requirements-docs.txt -r requirements-min.txt -r requirements-test.txt check # Runs formatters and code quality checkers over your workspace. This environment is lightweight compared to devenv because it installs dependencies once and completely skips installing a package from this repository as it does not need it. The operations performed by this environment are performed in place. Info This environment is lightweight, running tox -e check often is fine. Similarly to devenv it is bullet proof in comparison to other non-utility environments (mainly test runners). It should just install on demand, and every failure should be considered and fixed permanently. pyXY # Warning pyXY - test runner envs - they require special care and you are responsible for their well being. Executes full test suite with corresponding Python interpreter version, denoted by XX numbers. All available ones are: py37 py38 py39 py310 List of included dependencies: requirement.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 click> = 8.1.0,<8.2.0 matplotlib> = 3.5.0,<3.6.0 numpy> = 1.21.0,<1.22.0 packaging> = 21.3.0,<21.4.0 pandas> = 1.3.0,<1.4.0 rich> = 12.0.0,<12.1.0 scipy> = 1.7.0,<1.8.0 tensorflow> = 2.8.0,<2.9.0 pillow> = 9.1.0,<9.2.0 psutil> = 5.9.0,<5.10.0 py-cpuinfo> = 8.0.0,<8.1.0 pydantic> = 1.9.0,<1.10.0 backports.cached-property> = 1.0.2 requirement-test.txt 1 2 3 4 5 6 7 8 # Pytest + plugins pytest = =7.1.2 pytest-cov = =3.0.0 # Hypothesis https://hypothesis.readthedocs.io/en/latest/data.html hypothesis> = 6.46.0,<6.47.0 # Static typechecking mypy = =0.950 lxml = =4.8.0 mypy # Runs mypy over Python codebase to perform static type analysis. docs # Builds documentation with mkdocs, all generated files are saved to site/ folder. build-all # Builds package distribution wheels for corresponding Python version or all versions. build-all build-py37 build-py38 build-py39 build-py310 Environments with build prefix are responsible for building release packages for corresponding python versions ( build-py37 builds for Python 3.7 etc.) For each test environment ( py37 etc.) there is a corresponding build environment. Built packages (wheels) are stored in dist/ directory. Name tags list # 1 2 3 4 5 6 7 8 9 10 11 12 13 devenv docs check py37 py38 py39 py310 mypy build-all build-py37 build-py38 build-py39 build-py310","title":"Tox basics"},{"location":"develop/tox_basics/#tox-basics","text":"A virtual environment is a Python environment such that the Python interpreter, libraries and scripts installed into it are isolated from those installed in other virtual environments, and (by default) any libraries installed in a \u201csystem\u201d Python, i.e., one which is installed as part of your operating system.","title":"Tox basics"},{"location":"develop/tox_basics/#what-is-tox","text":"tox is a generic virtualenv management and test command line tool you can use for: checking that your package installs correctly with different Python versions and interpreters running your tests in each of the environments, configuring your test tool of choice acting as a frontend to Continuous Integration servers, greatly reducing boilerplate and merging CI and shell-based testing.","title":"What is tox?"},{"location":"develop/tox_basics/#handy-links","text":"To read more about tox, visit it's documentation. tox global settings tox environments configuration tox substitutions Generating environments, conditional settings Environmental variables Full tox cli documentation tox examples","title":"Handy Links"},{"location":"develop/tox_basics/#our-perspective","text":"Every complex Python project requires multiple tools for development and deployment. Those are mostly related to test suite running, checking quality of code, developing and building documentation and building distribution packages. Usually those are tedious tasks and they are at the top of the lists of tasks to automate. Here comes tox tox can be used as a reliable replacement for manually written scripts. It's designed to run predefined series of command in automatically generated Python virtual environment. It was designed for Python ecosystem and is widely used along Python projects. It is compatible with other Python tools out of the box. All the configuration is contained in tox.ini file and is completely static.","title":"Our perspective"},{"location":"develop/tox_basics/#basic-usage","text":"To invoke single environment with tox you have to memorize one simple command: 1 tox -e envname Where envname is replaced 1:1 with name of any environments listed below. Info You can also use tox command without any arguments to run checks for all supported python versions. Be aware that it is really time consuming.","title":"Basic usage"},{"location":"develop/tox_basics/#list-of-all-configured-environments","text":"Simplicity of creating tox managed environments allows us to create highly specialized environments with minimal boilerplate.","title":"List of all configured environments"},{"location":"develop/tox_basics/#devenv","text":"Stands for development environment (important when using IDE like Visual Studio Code or PyCharm). When selecting interpreter for your IDE, devenv is a right one to pick. This environment is meant to contain all tools important for continuos development including linters, formatters, building tools, packaging tools and everything else listed in requirements-dev.txt It is really heavy and expensive to create because of complexity of installation. Every call of tox -e devenv will completely recreate the environment. Danger Running tox -e devenv completely reinstalls environment - it's time consuming. It is designed in such way many due to the fact that during development there is no need to recreate it until something brakes, and then it's handy to simplify reinstallation how much possible. Hint Running this environment will install pre-commit. To select Python from devenv as interpreter in Visual Studio Code, use Ctrl + Shift + P and type Python: Select Interpreter , then hit Enter , select Enter interpreter path , pick Find and navigate to python.exe in .tox/devenv/bin (unix) or .tox/devev/scripts (windows). This environment is rather bullet proof in comparison to other non-utility environments (mainly test runners). It should just install on demand, and every failure should be considered and fixed permanently. List of included dependencies: requirement.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 click> = 8.1.0,<8.2.0 matplotlib> = 3.5.0,<3.6.0 numpy> = 1.21.0,<1.22.0 packaging> = 21.3.0,<21.4.0 pandas> = 1.3.0,<1.4.0 rich> = 12.0.0,<12.1.0 scipy> = 1.7.0,<1.8.0 tensorflow> = 2.8.0,<2.9.0 pillow> = 9.1.0,<9.2.0 psutil> = 5.9.0,<5.10.0 py-cpuinfo> = 8.0.0,<8.1.0 pydantic> = 1.9.0,<1.10.0 backports.cached-property> = 1.0.2 requirement-test.txt 1 2 3 4 5 6 7 8 # Pytest + plugins pytest = =7.1.2 pytest-cov = =3.0.0 # Hypothesis https://hypothesis.readthedocs.io/en/latest/data.html hypothesis> = 6.46.0,<6.47.0 # Static typechecking mypy = =0.950 lxml = =4.8.0 requirement-dev.txt 1 2 3 4 5 -r requirements.txt -r requirements-check.txt -r requirements-docs.txt -r requirements-min.txt -r requirements-test.txt","title":"devenv"},{"location":"develop/tox_basics/#check","text":"Runs formatters and code quality checkers over your workspace. This environment is lightweight compared to devenv because it installs dependencies once and completely skips installing a package from this repository as it does not need it. The operations performed by this environment are performed in place. Info This environment is lightweight, running tox -e check often is fine. Similarly to devenv it is bullet proof in comparison to other non-utility environments (mainly test runners). It should just install on demand, and every failure should be considered and fixed permanently.","title":"check"},{"location":"develop/tox_basics/#pyxy","text":"Warning pyXY - test runner envs - they require special care and you are responsible for their well being. Executes full test suite with corresponding Python interpreter version, denoted by XX numbers. All available ones are: py37 py38 py39 py310 List of included dependencies: requirement.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 click> = 8.1.0,<8.2.0 matplotlib> = 3.5.0,<3.6.0 numpy> = 1.21.0,<1.22.0 packaging> = 21.3.0,<21.4.0 pandas> = 1.3.0,<1.4.0 rich> = 12.0.0,<12.1.0 scipy> = 1.7.0,<1.8.0 tensorflow> = 2.8.0,<2.9.0 pillow> = 9.1.0,<9.2.0 psutil> = 5.9.0,<5.10.0 py-cpuinfo> = 8.0.0,<8.1.0 pydantic> = 1.9.0,<1.10.0 backports.cached-property> = 1.0.2 requirement-test.txt 1 2 3 4 5 6 7 8 # Pytest + plugins pytest = =7.1.2 pytest-cov = =3.0.0 # Hypothesis https://hypothesis.readthedocs.io/en/latest/data.html hypothesis> = 6.46.0,<6.47.0 # Static typechecking mypy = =0.950 lxml = =4.8.0","title":"pyXY"},{"location":"develop/tox_basics/#mypy","text":"Runs mypy over Python codebase to perform static type analysis.","title":"mypy"},{"location":"develop/tox_basics/#docs","text":"Builds documentation with mkdocs, all generated files are saved to site/ folder.","title":"docs"},{"location":"develop/tox_basics/#build-all","text":"Builds package distribution wheels for corresponding Python version or all versions. build-all build-py37 build-py38 build-py39 build-py310 Environments with build prefix are responsible for building release packages for corresponding python versions ( build-py37 builds for Python 3.7 etc.) For each test environment ( py37 etc.) there is a corresponding build environment. Built packages (wheels) are stored in dist/ directory.","title":"build-all"},{"location":"develop/tox_basics/#name-tags-list","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 devenv docs check py37 py38 py39 py310 mypy build-all build-py37 build-py38 build-py39 build-py310","title":"Name tags list"},{"location":"howto/env_in_vsc/","text":"Open VSCode in your repository folder Use Ctrl+Shift+P to open command entry Enter >Python: Select Interpreter and click Enter Select option \"Enter interpreter path...\" Select option \"Find...\" Navigate to Windows: NNEVE/.tox/devenv/Scripts/python.exe Linux: NNEVE/.tox/devenv/bin/python Select file python.exe Use Ctrl+Shift+P to open command entry Enter >Developer: Reload Window and click Enter Open terminal by pulling from edge of status bar Use trash can icon to kill it Repeat step 11. Now you should have (devenv) prefix before your command prompt If you don't have this prefix, manually activate environment: - Windows: \".tox/devenv/Script/activate\" - Linux: source \".tox/devenv/bin/activate\" - for more see this","title":"Set up environment in VSC"},{"location":"howto/set_up_vsc/","text":"HowTo set up Visual Studio Code for development # Make sure you have following extensions installed: Python Pylance YAML GitLens \u2014 Git supercharged indent-rainbow Test Explorer UI Test Explorer UI Use Ctrl+Shift+P to open VSC command prompt Enter >Preferences: Open Settings (UI) and click Enter to open Settings tab For all following use setting search field: Type Files: Auto Save and select option afterDelay Type Python: Language Server and select Pylance Type Files: Auto Save Delay and set value delay value to 1000 Type Python > Analysis: Diagnostic Mode and set value to workspace Type Python > Analysis: Extra Paths and use Add item to add ./source/ Type Python > Formatting: Provider and set it to black Type Python > Linting: Flake8 Enabled and check it Type Python > Linting: Pylint Enabled and uncheck it Type Docstring Format and select numpy Type Python > Analysis: Type Checking Mode and set to basic Type Indent Rainbow: Colors and check option Color On Whitespace Only : Type Indent Rainbow: Colors and in section Colors use Edit in settings.json to enter settings in json form, then use following to update \"indentRainbow.colors\" section: 1 2 3 4 5 6 7 8 9 \"indentRainbow.colors\": [ \"rgba(255,255,255,0.02)\", \"rgba(255,255,255,0.04)\", \"rgba(255,255,255,0.055)\", \"rgba(255,255,255,0.07)\", \"rgba(255,255,255,0.085)\", \"rgba(255,255,255,0.10)\", \"rgba(255,255,255,0.115)\" ]","title":"Set up Visual Studio Code"},{"location":"howto/set_up_vsc/#howto-set-up-visual-studio-code-for-development","text":"Make sure you have following extensions installed: Python Pylance YAML GitLens \u2014 Git supercharged indent-rainbow Test Explorer UI Test Explorer UI Use Ctrl+Shift+P to open VSC command prompt Enter >Preferences: Open Settings (UI) and click Enter to open Settings tab For all following use setting search field: Type Files: Auto Save and select option afterDelay Type Python: Language Server and select Pylance Type Files: Auto Save Delay and set value delay value to 1000 Type Python > Analysis: Diagnostic Mode and set value to workspace Type Python > Analysis: Extra Paths and use Add item to add ./source/ Type Python > Formatting: Provider and set it to black Type Python > Linting: Flake8 Enabled and check it Type Python > Linting: Pylint Enabled and uncheck it Type Docstring Format and select numpy Type Python > Analysis: Type Checking Mode and set to basic Type Indent Rainbow: Colors and check option Color On Whitespace Only : Type Indent Rainbow: Colors and in section Colors use Edit in settings.json to enter settings in json form, then use following to update \"indentRainbow.colors\" section: 1 2 3 4 5 6 7 8 9 \"indentRainbow.colors\": [ \"rgba(255,255,255,0.02)\", \"rgba(255,255,255,0.04)\", \"rgba(255,255,255,0.055)\", \"rgba(255,255,255,0.07)\", \"rgba(255,255,255,0.085)\", \"rgba(255,255,255,0.10)\", \"rgba(255,255,255,0.115)\" ]","title":"HowTo set up Visual Studio Code for development"},{"location":"howto/tox_devenv/","text":"Acquire Python interpreter version 3.8 from Python.org or with package manager. Downloads section Last Executable Windows Release Note Remember to add Python to PATH. Install tox with pip 1 python -m pip install tox Create virtual environment with tox: 1 python -m tox -e devenv Activate environment: Windows: \".tox/devenv/Scripts/activate\" Linux: source \".tox/devenv/bin/activate\" for more see this Info If you are using Powershell you may encounter this problem .","title":"Set up tox for development"},{"location":"quantum_oscillator/introduction/","text":"Introduction for Quantum Oscillator problem # The goal of this project was not so much to implement the network itself, but to verify how efficiently differential equations with an eigenproblem can be solved using neural networks. Unfortunately, as a result of this experiment, we came to the conclusion that the process of learning network is too time-consuming and in this form can not be applied to other problems that we considered as a target. As sad as it sounds, we leave the implementation of the network for all interested parties to see. On the other hand, we ourselves are changing the space of our interest from equations with an eigenproblem to equations without one. Maths behind the problem # Consider a mathematical problem of the following form $$ \\mathcal{L}f(x) = \\lambda f(x) $$ (1) \\(x\\) a variable \\(\\mathcal{L}\\) is a differential operator which depends on \\(x\\) and its derivatives \\(f(x)\\) is the eigenfunction \\(\\lambda\\) is the eigenvalue associated with eigenfunction Network input-output # Our neural network in form of represented by \\(N(x, \\lambda )\\) is expected to get two values as input: \\(x\\) - single floating point value within \\(x_L\\) to \\(x_R\\) range \\(\\lambda\\) - eigenvalue approximation selected by neural network As output, we also expect two values: \\(y\\) - floating point value corresponding to single \\(x\\) value within \\(x_L\\) to \\(x_R\\) range - \\(f(x)\\) approximation \\(\\lambda\\) - eigenvalue approximation selected by neural network To use our neural network as a replacement for \\(f(x)\\) we need to put the values returned from it into the equation below $$ f(x,\u03bb) = f_b + g(x)N(x,\u03bb) $$ (2) \\(f_b\\) arbitrary constant \\(g(x)\\) boundary condition \\(N(x, \\lambda )\\) - our neural network $$ g(x) = (1 \u2212e^{\u2212(x\u2212x_L)})(1 \u2212 e^{\u2212(x\u2212x_R)}) $$ (3) \\(x_L\\) minimum (left) value in the examined range \\(x_R\\) maximum (right) value in the examined range Loss function # $$ \\mathcal{L}f(x) - \u03bbf(x) = 0 $$ (4) By rearranging (1) to form (4) we are able to use it to measure how far from exact eigenfunction and exact eigenvalue is current state of neural network. To achieve this we have to replace \\(f(x)\\) with \\(f(x, \\lambda )\\) and use \\(\\lambda\\) retuned from network. Therefore without any prepared input data, just based on loss function and back propagation we can move from a random state of network to an approximation of the equation solution. Regularizators # To find multiple solutions of given equation with this neural network, it is necessary to add additional components to the loss function which will prevent the neural network from staying in trivial solution state and force transitions between successive states. $$ L_f = \\frac{1}{f(x, \\lambda )^2} $$ (5) - avoid trivial solutions $$ L_{\\lambda} = \\frac{1}{\\lambda^2} $$ (6) - avoid trivial solutions $$ L_{drive} = e ^ {-\\lambda + c} $$ (7) - push to subsequent solutions \\(c\\) - variable increased in regular intervals Problem we are solving # To make use of above theory we created network for solving Schrodinger's equation. In our case it takes following form $$ \\left[ \\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x) \\right] \\psi(x) = E\\psi(x) $$ (8) With \\(V(x)\\) defined as $$ \\frac{1}{2}kx^2 $$ (9) The exact solution of this equation has following form $$ \\psi _n(x) = \\frac{1}{\\sqrt{2^nn!}}\\frac{e^{-\\frac{x^2}{2}}}{\\pi ^ {\\frac{1}{4}}}H_n(x) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; E_n = n + \\frac{1}{2} $$ (10) Network structure # The network is a simple sequential model. The only unusual thing is returning a value from the lambda layer which is also the input to the network. This value is necessary to calculate the value of the loss function, and getting it in any other clever way breaks the whole learning process. Neural network structure Code flow # flowchart LR QOT((QOTracker)) QOC((QOConstants)) QON((QONetwork)) QOP((QOParams)) tr1[\"train_generations(...)\"] tr2[\"train(...)\"] QOT --> QOC --> QON --> tr1 QOP --> tr1 tr1 ---> tr2 ---> tr1","title":"Introduction"},{"location":"quantum_oscillator/introduction/#introduction-for-quantum-oscillator-problem","text":"The goal of this project was not so much to implement the network itself, but to verify how efficiently differential equations with an eigenproblem can be solved using neural networks. Unfortunately, as a result of this experiment, we came to the conclusion that the process of learning network is too time-consuming and in this form can not be applied to other problems that we considered as a target. As sad as it sounds, we leave the implementation of the network for all interested parties to see. On the other hand, we ourselves are changing the space of our interest from equations with an eigenproblem to equations without one.","title":"Introduction for Quantum Oscillator problem"},{"location":"quantum_oscillator/introduction/#maths-behind-the-problem","text":"Consider a mathematical problem of the following form $$ \\mathcal{L}f(x) = \\lambda f(x) $$ (1) \\(x\\) a variable \\(\\mathcal{L}\\) is a differential operator which depends on \\(x\\) and its derivatives \\(f(x)\\) is the eigenfunction \\(\\lambda\\) is the eigenvalue associated with eigenfunction","title":"Maths behind the problem"},{"location":"quantum_oscillator/introduction/#network-input-output","text":"Our neural network in form of represented by \\(N(x, \\lambda )\\) is expected to get two values as input: \\(x\\) - single floating point value within \\(x_L\\) to \\(x_R\\) range \\(\\lambda\\) - eigenvalue approximation selected by neural network As output, we also expect two values: \\(y\\) - floating point value corresponding to single \\(x\\) value within \\(x_L\\) to \\(x_R\\) range - \\(f(x)\\) approximation \\(\\lambda\\) - eigenvalue approximation selected by neural network To use our neural network as a replacement for \\(f(x)\\) we need to put the values returned from it into the equation below $$ f(x,\u03bb) = f_b + g(x)N(x,\u03bb) $$ (2) \\(f_b\\) arbitrary constant \\(g(x)\\) boundary condition \\(N(x, \\lambda )\\) - our neural network $$ g(x) = (1 \u2212e^{\u2212(x\u2212x_L)})(1 \u2212 e^{\u2212(x\u2212x_R)}) $$ (3) \\(x_L\\) minimum (left) value in the examined range \\(x_R\\) maximum (right) value in the examined range","title":"Network input-output"},{"location":"quantum_oscillator/introduction/#loss-function","text":"$$ \\mathcal{L}f(x) - \u03bbf(x) = 0 $$ (4) By rearranging (1) to form (4) we are able to use it to measure how far from exact eigenfunction and exact eigenvalue is current state of neural network. To achieve this we have to replace \\(f(x)\\) with \\(f(x, \\lambda )\\) and use \\(\\lambda\\) retuned from network. Therefore without any prepared input data, just based on loss function and back propagation we can move from a random state of network to an approximation of the equation solution.","title":"Loss function"},{"location":"quantum_oscillator/introduction/#regularizators","text":"To find multiple solutions of given equation with this neural network, it is necessary to add additional components to the loss function which will prevent the neural network from staying in trivial solution state and force transitions between successive states. $$ L_f = \\frac{1}{f(x, \\lambda )^2} $$ (5) - avoid trivial solutions $$ L_{\\lambda} = \\frac{1}{\\lambda^2} $$ (6) - avoid trivial solutions $$ L_{drive} = e ^ {-\\lambda + c} $$ (7) - push to subsequent solutions \\(c\\) - variable increased in regular intervals","title":"Regularizators"},{"location":"quantum_oscillator/introduction/#problem-we-are-solving","text":"To make use of above theory we created network for solving Schrodinger's equation. In our case it takes following form $$ \\left[ \\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x) \\right] \\psi(x) = E\\psi(x) $$ (8) With \\(V(x)\\) defined as $$ \\frac{1}{2}kx^2 $$ (9) The exact solution of this equation has following form $$ \\psi _n(x) = \\frac{1}{\\sqrt{2^nn!}}\\frac{e^{-\\frac{x^2}{2}}}{\\pi ^ {\\frac{1}{4}}}H_n(x) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; E_n = n + \\frac{1}{2} $$ (10)","title":"Problem we are solving"},{"location":"quantum_oscillator/introduction/#network-structure","text":"The network is a simple sequential model. The only unusual thing is returning a value from the lambda layer which is also the input to the network. This value is necessary to calculate the value of the loss function, and getting it in any other clever way breaks the whole learning process. Neural network structure","title":"Network structure"},{"location":"quantum_oscillator/introduction/#code-flow","text":"flowchart LR QOT((QOTracker)) QOC((QOConstants)) QON((QONetwork)) QOP((QOParams)) tr1[\"train_generations(...)\"] tr2[\"train(...)\"] QOT --> QOC --> QON --> tr1 QOP --> tr1 tr1 ---> tr2 ---> tr1","title":"Code flow"},{"location":"quantum_oscillator/learning_cycle/","text":"How to run QONetwork learning cycle # To run learning cycle, first you have to instantiate QOConstants object containing parameters of QONetwork and QOTracker for tracking learning process metrics. 1 2 3 4 5 6 7 8 9 constants = QOConstants ( k = 4.0 , mass = 1.0 , x_left =- 6.0 , x_right = 6.0 , fb = 0.0 , sample_size = 1200 , tracker = QOTracker (), ) Then you have to instantiate QONetwork object 1 network = QONetwork ( constants = constants ) And after above steps you can run learning loop 1 2 3 4 5 6 7 8 for index , nn in enumerate ( network . train_generations ( QOParams ( c =- 2.0 , c_step = 0.16 ), generations = 150 , epochs = 1000 , ) ): pass After each generation of learning, body of loop will be executed, thus you can stuff any kind of plotting there. For example you can use QOTracker.plot() (see full code snippet at the very bottom) Example learning graph created using QOTracker.plot() Full code snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import gc from pathlib import Path import matplotlib import tensorflow as tf from matplotlib import pyplot as plt from nneve.quantum_oscillator import ( QOConstants , QONetwork , QOParams , QOTracker , ) from nneve.utility.testing import disable_gpu_or_skip EXAMPLES_CODE = Path ( __file__ ) . parent EXAMPLES_DIR = EXAMPLES_CODE . parent WEIGHTS_DIR = EXAMPLES_DIR / \"weights\" PLOTS_DIR = EXAMPLES_DIR / \"plots\" tf . random . set_seed ( 0 ) disable_gpu_or_skip () constants = QOConstants ( k = 4.0 , mass = 1.0 , x_left =- 6.0 , x_right = 6.0 , fb = 0.0 , sample_size = 1200 , tracker = QOTracker (), ) network = QONetwork ( constants = constants , is_debug = True ) network . summary () matplotlib . use ( \"Agg\" ) for index , nn in enumerate ( network . train_generations ( QOParams ( c =- 2.0 , c_step = 0.16 ), generations = 150 , epochs = 1000 , ) ): x = nn . constants . get_sample () y2 , _ = nn ( x ) nn . constants . tracker . plot ( y2 , x ) # savefig tends to create memory leaks plt . savefig ( PLOTS_DIR / f \" { index } .png\" ) plt . cla () plt . clf () plt . close ( \"all\" ) gc . collect () nn . save ( WEIGHTS_DIR / f \" { index } .w\" )","title":"Learning cycle"},{"location":"quantum_oscillator/learning_cycle/#how-to-run-qonetwork-learning-cycle","text":"To run learning cycle, first you have to instantiate QOConstants object containing parameters of QONetwork and QOTracker for tracking learning process metrics. 1 2 3 4 5 6 7 8 9 constants = QOConstants ( k = 4.0 , mass = 1.0 , x_left =- 6.0 , x_right = 6.0 , fb = 0.0 , sample_size = 1200 , tracker = QOTracker (), ) Then you have to instantiate QONetwork object 1 network = QONetwork ( constants = constants ) And after above steps you can run learning loop 1 2 3 4 5 6 7 8 for index , nn in enumerate ( network . train_generations ( QOParams ( c =- 2.0 , c_step = 0.16 ), generations = 150 , epochs = 1000 , ) ): pass After each generation of learning, body of loop will be executed, thus you can stuff any kind of plotting there. For example you can use QOTracker.plot() (see full code snippet at the very bottom) Example learning graph created using QOTracker.plot() Full code snippet: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import gc from pathlib import Path import matplotlib import tensorflow as tf from matplotlib import pyplot as plt from nneve.quantum_oscillator import ( QOConstants , QONetwork , QOParams , QOTracker , ) from nneve.utility.testing import disable_gpu_or_skip EXAMPLES_CODE = Path ( __file__ ) . parent EXAMPLES_DIR = EXAMPLES_CODE . parent WEIGHTS_DIR = EXAMPLES_DIR / \"weights\" PLOTS_DIR = EXAMPLES_DIR / \"plots\" tf . random . set_seed ( 0 ) disable_gpu_or_skip () constants = QOConstants ( k = 4.0 , mass = 1.0 , x_left =- 6.0 , x_right = 6.0 , fb = 0.0 , sample_size = 1200 , tracker = QOTracker (), ) network = QONetwork ( constants = constants , is_debug = True ) network . summary () matplotlib . use ( \"Agg\" ) for index , nn in enumerate ( network . train_generations ( QOParams ( c =- 2.0 , c_step = 0.16 ), generations = 150 , epochs = 1000 , ) ): x = nn . constants . get_sample () y2 , _ = nn ( x ) nn . constants . tracker . plot ( y2 , x ) # savefig tends to create memory leaks plt . savefig ( PLOTS_DIR / f \" { index } .png\" ) plt . cla () plt . clf () plt . close ( \"all\" ) gc . collect () nn . save ( WEIGHTS_DIR / f \" { index } .w\" )","title":"How to run QONetwork learning cycle"},{"location":"quantum_oscillator/reference/constants/","text":"class QOConstants ( pydantic . BaseModel ) # Parent classes # class pydantic . BaseModel Introduction # class QOConstants contains physical constants used in loss function during neural network learning process. It inherits from the pydantic's BaseModel class to guarantee field type compatibility and their correct filling without manual implementation of all checks. Example construction with manual values set 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from tensorflow import keras from nneve.quantum_oscillator import QOConstants , QOTracker constants = QOConstants ( optimizer = keras . optimizers . Adam (), tracker = QOTracker (), k = 4.0 , mass = 1.0 , x_left =- 6.0 , x_right = 6.0 , fb = 1.0 , sample_size = 500 , v_f = 1.0 , v_lambda = 1.0 , v_drive = 1.0 , ) Instance attributes # Note Attributes are mutable Arbitrary types are allowed to be used as attribute values optimizer : keras . optimizers . Optimizer # 1 2 3 4 5 6 7 Field ( default = keras . optimizers . Adam ( learning_rate = DEFAULT_LEARNING_RATE , beta_1 = DEFAULT_BETA_1 , beta_2 = DEFAULT_BETA_2 , ) ) Argument required for compiling a Keras model. tracker : QOTracker # 1 Field ( default_factory = QOTracker ) QOTracker class, responsible for collecting metrics during neural network learning process. k : float # 1 Field ( default = 4.0 ) Oscillator force constant. mass : float # 1 Field ( default = 1.0 ) Oscillator mass used in \\([\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x)]\\psi(x) = E\\psi(x)\\) x_left : float # 1 Field ( default =- 6.0 ) Left boundary condition of our quantum harmonic oscillator model. x_right : float # 1 Field ( default = 6.0 ) Right boundary condition of our quantum harmonic oscillator model. fb : float # 1 Field ( default = 0.0 ) Constant boundary value for boundary conditions. sample_size : int # 1 Field ( default = 1000 ) Size of our current learning sample (number of points on the linear space). v_f : int # 1 Field ( default = 1.0 ) Multiplier of regularization function which prevents our network from learning trivial eigenfunctions. v_lambda : int # 1 Field ( default = 1.0 ) Multiplier of regularization function which prevents our network from learning trivial eigenvalues. v_drive : int # 1 Field ( default = 1.0 ) Multiplier of regularization function which motivates our network to scan for higher values of eigenvalues. Instance methods # def sample ( self ) -> tf . Tensor # Generates tensor of sample_size float32 values in range from x_left to x_right for network learning process. Returns # type description tf.Tensor float32 tensor in shape (sample_size, 1)","title":"class QOConstants"},{"location":"quantum_oscillator/reference/constants/#class-qoconstantspydanticbasemodel","text":"","title":"class QOConstants(pydantic.BaseModel)"},{"location":"quantum_oscillator/reference/constants/#parent-classes","text":"class pydantic . BaseModel","title":"Parent classes"},{"location":"quantum_oscillator/reference/constants/#introduction","text":"class QOConstants contains physical constants used in loss function during neural network learning process. It inherits from the pydantic's BaseModel class to guarantee field type compatibility and their correct filling without manual implementation of all checks. Example construction with manual values set 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from tensorflow import keras from nneve.quantum_oscillator import QOConstants , QOTracker constants = QOConstants ( optimizer = keras . optimizers . Adam (), tracker = QOTracker (), k = 4.0 , mass = 1.0 , x_left =- 6.0 , x_right = 6.0 , fb = 1.0 , sample_size = 500 , v_f = 1.0 , v_lambda = 1.0 , v_drive = 1.0 , )","title":"Introduction"},{"location":"quantum_oscillator/reference/constants/#instance-attributes","text":"Note Attributes are mutable Arbitrary types are allowed to be used as attribute values","title":"Instance attributes"},{"location":"quantum_oscillator/reference/constants/#optimizer-kerasoptimizersoptimizer","text":"1 2 3 4 5 6 7 Field ( default = keras . optimizers . Adam ( learning_rate = DEFAULT_LEARNING_RATE , beta_1 = DEFAULT_BETA_1 , beta_2 = DEFAULT_BETA_2 , ) ) Argument required for compiling a Keras model.","title":"optimizer: keras.optimizers.Optimizer"},{"location":"quantum_oscillator/reference/constants/#tracker-qotracker","text":"1 Field ( default_factory = QOTracker ) QOTracker class, responsible for collecting metrics during neural network learning process.","title":"tracker: QOTracker"},{"location":"quantum_oscillator/reference/constants/#k-float","text":"1 Field ( default = 4.0 ) Oscillator force constant.","title":"k: float"},{"location":"quantum_oscillator/reference/constants/#mass-float","text":"1 Field ( default = 1.0 ) Oscillator mass used in \\([\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x)]\\psi(x) = E\\psi(x)\\)","title":"mass: float"},{"location":"quantum_oscillator/reference/constants/#x_left-float","text":"1 Field ( default =- 6.0 ) Left boundary condition of our quantum harmonic oscillator model.","title":"x_left: float"},{"location":"quantum_oscillator/reference/constants/#x_right-float","text":"1 Field ( default = 6.0 ) Right boundary condition of our quantum harmonic oscillator model.","title":"x_right: float"},{"location":"quantum_oscillator/reference/constants/#fb-float","text":"1 Field ( default = 0.0 ) Constant boundary value for boundary conditions.","title":"fb: float"},{"location":"quantum_oscillator/reference/constants/#sample_size-int","text":"1 Field ( default = 1000 ) Size of our current learning sample (number of points on the linear space).","title":"sample_size: int"},{"location":"quantum_oscillator/reference/constants/#v_f-int","text":"1 Field ( default = 1.0 ) Multiplier of regularization function which prevents our network from learning trivial eigenfunctions.","title":"v_f: int"},{"location":"quantum_oscillator/reference/constants/#v_lambda-int","text":"1 Field ( default = 1.0 ) Multiplier of regularization function which prevents our network from learning trivial eigenvalues.","title":"v_lambda: int"},{"location":"quantum_oscillator/reference/constants/#v_drive-int","text":"1 Field ( default = 1.0 ) Multiplier of regularization function which motivates our network to scan for higher values of eigenvalues.","title":"v_drive: int"},{"location":"quantum_oscillator/reference/constants/#instance-methods","text":"","title":"Instance methods"},{"location":"quantum_oscillator/reference/constants/#def-sampleself-tftensor","text":"Generates tensor of sample_size float32 values in range from x_left to x_right for network learning process.","title":"def sample(self) -&gt; tf.Tensor"},{"location":"quantum_oscillator/reference/constants/#returns","text":"type description tf.Tensor float32 tensor in shape (sample_size, 1)","title":"Returns"},{"location":"quantum_oscillator/reference/network/","text":"class QONetwork ( keras . Model ) # Introduction # This class contains actual implementation of neural network for solving quantum oscillator problem. Bases: keras . Model Source code in nneve/quantum_oscillator/network.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 class QONetwork ( keras . Model ): constants : QOConstants is_debug : bool is_console_mode : bool def __init__ ( self , constants : Optional [ QOConstants ] = None , is_debug : bool = False , is_console_mode : bool = True , name : str = \"QONetwork\" , ): \"\"\"Initialize quantum oscillator properties, neural network shape and create loss function object. Parameters ---------- constants : Optional[QOConstants], optional Constant values describing quantum oscillator, by default None is_debug : bool, optional debug mode switch, by default False is_console_mode : bool, optional When true, rich.Progress will be used to manifest learning process progress, by default True name : str, optional Aesthetic only, name for model, by default \"QONetwork\" \"\"\" self . constants = constants if constants is not None else QOConstants () self . is_console_mode = is_console_mode self . is_debug = is_debug # attribute access to cache function value self . loss_function # create network structure with two 50 neuron deep layers inputs , outputs = self . assemble_hook (( 50 , 50 )) # with constructor of keras.Model initialize model machinery super () . __init__ ( inputs = inputs , outputs = outputs , name = name , ) # not really needed thanks to custom train loop # but is called to satisfy internals of keras self . compile ( self . constants . optimizer , jit_compile = True , ) assert self . name is not None def assemble_hook ( self , deep_layers : Sequence [ int ] = ( 50 , 50 ) ) -> Tuple [ List [ keras . layers . InputLayer ], List [ keras . layers . Dense ]]: \"\"\"Construct neural network structure with specified number of deep layers used for transformation of input. Parameters ---------- deep_layers : Sequence[int], optional Number and size of deep layers, by default (50, 50) Returns ------- Tuple[List[keras.layers.InputLayer], List[keras.layers.Dense]] Input and output of neural network. \"\"\" # 1 value input layer inputs = cast ( keras . layers . InputLayer , keras . Input ( shape = ( 1 ,), name = \"input\" , dtype = tf . float32 , ), ) # One neuron decides what value should \u03bb have eigenvalue_out = Eigenvalue ( name = \"eigenvalue\" )( inputs ) input_and_eigenvalue = keras . layers . Concatenate ( axis = 1 , name = \"join\" , )([ inputs , eigenvalue_out ]) # dense layer seem not to be willing to accept two inputs # at the same time, therefore \u03bb and x are bound together # with help of another dense layer d1 = keras . layers . Dense ( 2 , activation = tf . sin , name = \"dense_input\" , dtype = tf . float32 , )( input_and_eigenvalue ) # dynamically defined number of deep layers, specified by # input argument `deep_layers`. They are joined sequentially deep_in = d1 deep_out = None for index , neuron_count in enumerate ( deep_layers ): deep_out = keras . layers . Dense ( neuron_count , activation = tf . sin , name = f \"dense_ { index } \" , dtype = tf . float32 , )( deep_in ) deep_in = deep_out del index # make sure no references by mistake # We require at least one deep layer to be available, otherwise # this network makes no sense assert deep_out is not None del deep_in # make sure no references by mistake # single value output from neural network outputs = keras . layers . Dense ( 1 , name = \"predictions\" , dtype = tf . float32 , )( deep_out ) # eigenvalue is not acquired by single layer call because it # was breaking learning process return [ inputs ], [ outputs , eigenvalue_out ] @cached_property def loss_function ( self ) -> LossFunctionT : # noqa: CFQ004, CFQ001 \"\"\"Create and return loss function used for learning process. Returns ------- LossFunctionT loss function object. \"\"\" @tf . function # type: ignore def loss_function_impl ( x : tf . Tensor , c : tf . Tensor , ) -> Tuple [ tf . Tensor , Tuple [ Any , ... ]]: # pragma: no cover # for more extensive description of loss function visit # https://argmaster.github.io/NNEVE/quantum_oscillator/introduction/ current_eigenvalue = self ( x )[ 1 ][ 0 ] deriv_x = tf . identity ( self . constants . get_sample ()) with tf . GradientTape () as second : second . watch ( deriv_x ) with tf . GradientTape () as first : first . watch ( deriv_x ) psi , _ = parametric_solution ( deriv_x ) # type: ignore dy_dx = first . gradient ( psi , deriv_x ) dy_dxx = second . gradient ( dy_dx , deriv_x ) residuum = tf . square ( tf . divide ( dy_dxx , - 2.0 * self . constants . mass ) + ( potential ( x ) * psi ) - ( current_eigenvalue * psi ) ) function_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( psi )), 1e-6 ), ) lambda_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( current_eigenvalue )), 1e-6 ) ) drive_loss = tf . exp ( tf . reduce_mean ( tf . subtract ( c , current_eigenvalue )) ) total_loss = residuum + function_loss + lambda_loss + drive_loss return total_loss , ( tf . reduce_mean ( total_loss ), current_eigenvalue , tf . reduce_mean ( residuum ), function_loss , lambda_loss , drive_loss , c , ) # type: ignore @tf . function # type: ignore def parametric_solution ( x : tf . Variable , ) -> Tuple [ tf . Tensor , tf . Tensor ]: # pragma: no cover psi , current_eigenvalue = self ( x ) return ( tf . add ( tf . constant ( self . constants . fb , dtype = tf . float32 ), tf . multiply ( boundary ( x ), psi ), ), current_eigenvalue [ 0 ], ) @tf . function # type: ignore def boundary ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return ( 1 - tf . exp ( tf . subtract ( self . constants . x_left , x ))) * ( 1 - tf . exp ( tf . subtract ( x , self . constants . x_right )) ) @tf . function # type: ignore def potential ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return tf . divide ( tf . multiply ( self . constants . k , tf . square ( x )), 2 ) if self . is_debug : self . debug_potential_function = potential self . debug_boundary_function = boundary self . debug_parametric_solution_function = parametric_solution self . debug_loss_function_function = loss_function_impl self . parametric_solution = parametric_solution return cast ( LossFunctionT , loss_function_impl ) def train_generations ( self , params : QOParams , generations : int = 5 , epochs : int = 1000 ) -> Iterable [ \"QONetwork\" ]: \"\"\"Train multiple generations of neural network. For each generation train() method is called, followed by update of QOParams. Parameters ---------- params : QOParams parameters describing quantum oscillator generations : int, optional number of generations to calculate, by default 5 epochs : int, optional number of epochs in each generation, by default 1000 Yields ------ Iterable[QONetwork] This neural network after generation is completed. \"\"\" with suppress ( KeyboardInterrupt ): for i in range ( generations ): logging . info ( f \"Generation: { i + 1 : 4.0f } our of \" f \" { generations : .0f } , ( { i / generations : .2% } )\" ) self . train ( params , epochs ) params . update () yield self def train ( # noqa: CCR001 self , params : QOParams , epochs : int = 10 ) -> None : \"\"\"Train single generation of neural network. Single generation consists of predefined number of epochs, each containing call to loss function and following improvement of neural network weights. Parameters ---------- params : QOParams parameters describing quantum oscillator epochs : int, optional number of epochs to calculate, by default 10 \"\"\" x = self . constants . get_sample () if self . is_console_mode : with Progress () as progress : task = progress . add_task ( description = \"Learning...\" , total = epochs + 1 ) for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) progress . update ( task , advance = 1 , description = description . capitalize (), ) else : for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) logging . info ( description ) def train_step ( self , x : tf . Tensor , params : QOParams ) -> float : \"\"\"Step of learning process. Step consists of single call to loss function and following improvement of network weights. Parameters ---------- x : tf.Tensor tensor containing X values grid. params : QOParams parameters describing quantum oscillator. Returns ------- float average loss of network before current improvement. \"\"\" with tf . GradientTape () as tape : loss_value , stats = self . loss_function ( x , * params . get_extra ()) trainable_vars = self . trainable_variables gradients = tape . gradient ( loss_value , trainable_vars ) self . constants . optimizer . apply_gradients ( zip ( gradients , trainable_vars ) ) # to make this push loss function agnostic average_loss = tf . reduce_mean ( loss_value ) self . constants . tracker . push_stats ( * stats ) return float ( average_loss ) def save ( self , filepath : Path ) -> None : # noqa: FNE003 \"\"\"Save model object to file. Parameters ---------- filepath : Path Path to file where model data should be saved. If file exists, it will be overwritten. \"\"\" weights = self . get_weights () with filepath . open ( \"wb\" ) as file : pickle . dump ( weights , file ) def load ( self , filepath : Path ) -> None : # noqa: FNE004 \"\"\"Load model data from file. Parameters ---------- filepath : Path Path to file where model data is stored. \"\"\" with filepath . open ( \"rb\" ) as file : weights = pickle . load ( file ) self . set_weights ( weights ) def plot_solution ( self ) -> None : \"\"\"Generate a plot with pyplot.plot which shows solution currently proposed by neural network.\"\"\" x = self . constants . get_sample () plt . plot ( x , self ( x )[ 0 ]) __init__ ( constants = None , is_debug = False , is_console_mode = True , name = 'QONetwork' ) # Initialize quantum oscillator properties, neural network shape and create loss function object. Parameters: Name Type Description Default constants Optional [ QOConstants ], optional Constant values describing quantum oscillator, by default None None is_debug bool , optional debug mode switch, by default False False is_console_mode bool , optional When true, rich.Progress will be used to manifest learning process progress, by default True True name str , optional Aesthetic only, name for model, by default \"QONetwork\" 'QONetwork' Source code in nneve/quantum_oscillator/network.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , constants : Optional [ QOConstants ] = None , is_debug : bool = False , is_console_mode : bool = True , name : str = \"QONetwork\" , ): \"\"\"Initialize quantum oscillator properties, neural network shape and create loss function object. Parameters ---------- constants : Optional[QOConstants], optional Constant values describing quantum oscillator, by default None is_debug : bool, optional debug mode switch, by default False is_console_mode : bool, optional When true, rich.Progress will be used to manifest learning process progress, by default True name : str, optional Aesthetic only, name for model, by default \"QONetwork\" \"\"\" self . constants = constants if constants is not None else QOConstants () self . is_console_mode = is_console_mode self . is_debug = is_debug # attribute access to cache function value self . loss_function # create network structure with two 50 neuron deep layers inputs , outputs = self . assemble_hook (( 50 , 50 )) # with constructor of keras.Model initialize model machinery super () . __init__ ( inputs = inputs , outputs = outputs , name = name , ) # not really needed thanks to custom train loop # but is called to satisfy internals of keras self . compile ( self . constants . optimizer , jit_compile = True , ) assert self . name is not None assemble_hook ( deep_layers = ( 50 , 50 )) # Construct neural network structure with specified number of deep layers used for transformation of input. Parameters: Name Type Description Default deep_layers Sequence [ int ], optional Number and size of deep layers, by default (50, 50) (50, 50) Returns: Type Description Tuple [ List [ keras . layers . InputLayer ], List [ keras . layers . Dense ]] Input and output of neural network. Source code in nneve/quantum_oscillator/network.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def assemble_hook ( self , deep_layers : Sequence [ int ] = ( 50 , 50 ) ) -> Tuple [ List [ keras . layers . InputLayer ], List [ keras . layers . Dense ]]: \"\"\"Construct neural network structure with specified number of deep layers used for transformation of input. Parameters ---------- deep_layers : Sequence[int], optional Number and size of deep layers, by default (50, 50) Returns ------- Tuple[List[keras.layers.InputLayer], List[keras.layers.Dense]] Input and output of neural network. \"\"\" # 1 value input layer inputs = cast ( keras . layers . InputLayer , keras . Input ( shape = ( 1 ,), name = \"input\" , dtype = tf . float32 , ), ) # One neuron decides what value should \u03bb have eigenvalue_out = Eigenvalue ( name = \"eigenvalue\" )( inputs ) input_and_eigenvalue = keras . layers . Concatenate ( axis = 1 , name = \"join\" , )([ inputs , eigenvalue_out ]) # dense layer seem not to be willing to accept two inputs # at the same time, therefore \u03bb and x are bound together # with help of another dense layer d1 = keras . layers . Dense ( 2 , activation = tf . sin , name = \"dense_input\" , dtype = tf . float32 , )( input_and_eigenvalue ) # dynamically defined number of deep layers, specified by # input argument `deep_layers`. They are joined sequentially deep_in = d1 deep_out = None for index , neuron_count in enumerate ( deep_layers ): deep_out = keras . layers . Dense ( neuron_count , activation = tf . sin , name = f \"dense_ { index } \" , dtype = tf . float32 , )( deep_in ) deep_in = deep_out del index # make sure no references by mistake # We require at least one deep layer to be available, otherwise # this network makes no sense assert deep_out is not None del deep_in # make sure no references by mistake # single value output from neural network outputs = keras . layers . Dense ( 1 , name = \"predictions\" , dtype = tf . float32 , )( deep_out ) # eigenvalue is not acquired by single layer call because it # was breaking learning process return [ inputs ], [ outputs , eigenvalue_out ] load ( filepath ) # Load model data from file. Parameters: Name Type Description Default filepath Path Path to file where model data is stored. required Source code in nneve/quantum_oscillator/network.py 359 360 361 362 363 364 365 366 367 368 369 def load ( self , filepath : Path ) -> None : # noqa: FNE004 \"\"\"Load model data from file. Parameters ---------- filepath : Path Path to file where model data is stored. \"\"\" with filepath . open ( \"rb\" ) as file : weights = pickle . load ( file ) self . set_weights ( weights ) loss_function () property cached # Create and return loss function used for learning process. Returns: Type Description LossFunctionT loss function object. Source code in nneve/quantum_oscillator/network.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 @cached_property def loss_function ( self ) -> LossFunctionT : # noqa: CFQ004, CFQ001 \"\"\"Create and return loss function used for learning process. Returns ------- LossFunctionT loss function object. \"\"\" @tf . function # type: ignore def loss_function_impl ( x : tf . Tensor , c : tf . Tensor , ) -> Tuple [ tf . Tensor , Tuple [ Any , ... ]]: # pragma: no cover # for more extensive description of loss function visit # https://argmaster.github.io/NNEVE/quantum_oscillator/introduction/ current_eigenvalue = self ( x )[ 1 ][ 0 ] deriv_x = tf . identity ( self . constants . get_sample ()) with tf . GradientTape () as second : second . watch ( deriv_x ) with tf . GradientTape () as first : first . watch ( deriv_x ) psi , _ = parametric_solution ( deriv_x ) # type: ignore dy_dx = first . gradient ( psi , deriv_x ) dy_dxx = second . gradient ( dy_dx , deriv_x ) residuum = tf . square ( tf . divide ( dy_dxx , - 2.0 * self . constants . mass ) + ( potential ( x ) * psi ) - ( current_eigenvalue * psi ) ) function_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( psi )), 1e-6 ), ) lambda_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( current_eigenvalue )), 1e-6 ) ) drive_loss = tf . exp ( tf . reduce_mean ( tf . subtract ( c , current_eigenvalue )) ) total_loss = residuum + function_loss + lambda_loss + drive_loss return total_loss , ( tf . reduce_mean ( total_loss ), current_eigenvalue , tf . reduce_mean ( residuum ), function_loss , lambda_loss , drive_loss , c , ) # type: ignore @tf . function # type: ignore def parametric_solution ( x : tf . Variable , ) -> Tuple [ tf . Tensor , tf . Tensor ]: # pragma: no cover psi , current_eigenvalue = self ( x ) return ( tf . add ( tf . constant ( self . constants . fb , dtype = tf . float32 ), tf . multiply ( boundary ( x ), psi ), ), current_eigenvalue [ 0 ], ) @tf . function # type: ignore def boundary ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return ( 1 - tf . exp ( tf . subtract ( self . constants . x_left , x ))) * ( 1 - tf . exp ( tf . subtract ( x , self . constants . x_right )) ) @tf . function # type: ignore def potential ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return tf . divide ( tf . multiply ( self . constants . k , tf . square ( x )), 2 ) if self . is_debug : self . debug_potential_function = potential self . debug_boundary_function = boundary self . debug_parametric_solution_function = parametric_solution self . debug_loss_function_function = loss_function_impl self . parametric_solution = parametric_solution return cast ( LossFunctionT , loss_function_impl ) plot_solution () # Generate a plot with pyplot.plot which shows solution currently proposed by neural network. Source code in nneve/quantum_oscillator/network.py 371 372 373 374 375 def plot_solution ( self ) -> None : \"\"\"Generate a plot with pyplot.plot which shows solution currently proposed by neural network.\"\"\" x = self . constants . get_sample () plt . plot ( x , self ( x )[ 0 ]) save ( filepath ) # Save model object to file. Parameters: Name Type Description Default filepath Path Path to file where model data should be saved. If file exists, it will be overwritten. required Source code in nneve/quantum_oscillator/network.py 346 347 348 349 350 351 352 353 354 355 356 357 def save ( self , filepath : Path ) -> None : # noqa: FNE003 \"\"\"Save model object to file. Parameters ---------- filepath : Path Path to file where model data should be saved. If file exists, it will be overwritten. \"\"\" weights = self . get_weights () with filepath . open ( \"wb\" ) as file : pickle . dump ( weights , file ) train ( params , epochs = 10 ) # Train single generation of neural network. Single generation consists of predefined number of epochs, each containing call to loss function and following improvement of neural network weights. Parameters: Name Type Description Default params QOParams parameters describing quantum oscillator required epochs int , optional number of epochs to calculate, by default 10 10 Source code in nneve/quantum_oscillator/network.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def train ( # noqa: CCR001 self , params : QOParams , epochs : int = 10 ) -> None : \"\"\"Train single generation of neural network. Single generation consists of predefined number of epochs, each containing call to loss function and following improvement of neural network weights. Parameters ---------- params : QOParams parameters describing quantum oscillator epochs : int, optional number of epochs to calculate, by default 10 \"\"\" x = self . constants . get_sample () if self . is_console_mode : with Progress () as progress : task = progress . add_task ( description = \"Learning...\" , total = epochs + 1 ) for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) progress . update ( task , advance = 1 , description = description . capitalize (), ) else : for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) logging . info ( description ) train_generations ( params , generations = 5 , epochs = 1000 ) # Train multiple generations of neural network. For each generation train() method is called, followed by update of QOParams. Parameters: Name Type Description Default params QOParams parameters describing quantum oscillator required generations int , optional number of generations to calculate, by default 5 5 epochs int , optional number of epochs in each generation, by default 1000 1000 Yields: Type Description Iterable [ QONetwork ] This neural network after generation is completed. Source code in nneve/quantum_oscillator/network.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def train_generations ( self , params : QOParams , generations : int = 5 , epochs : int = 1000 ) -> Iterable [ \"QONetwork\" ]: \"\"\"Train multiple generations of neural network. For each generation train() method is called, followed by update of QOParams. Parameters ---------- params : QOParams parameters describing quantum oscillator generations : int, optional number of generations to calculate, by default 5 epochs : int, optional number of epochs in each generation, by default 1000 Yields ------ Iterable[QONetwork] This neural network after generation is completed. \"\"\" with suppress ( KeyboardInterrupt ): for i in range ( generations ): logging . info ( f \"Generation: { i + 1 : 4.0f } our of \" f \" { generations : .0f } , ( { i / generations : .2% } )\" ) self . train ( params , epochs ) params . update () yield self train_step ( x , params ) # Step of learning process. Step consists of single call to loss function and following improvement of network weights. Parameters: Name Type Description Default x tf . Tensor tensor containing X values grid. required params QOParams parameters describing quantum oscillator. required Returns: Type Description float average loss of network before current improvement. Source code in nneve/quantum_oscillator/network.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def train_step ( self , x : tf . Tensor , params : QOParams ) -> float : \"\"\"Step of learning process. Step consists of single call to loss function and following improvement of network weights. Parameters ---------- x : tf.Tensor tensor containing X values grid. params : QOParams parameters describing quantum oscillator. Returns ------- float average loss of network before current improvement. \"\"\" with tf . GradientTape () as tape : loss_value , stats = self . loss_function ( x , * params . get_extra ()) trainable_vars = self . trainable_variables gradients = tape . gradient ( loss_value , trainable_vars ) self . constants . optimizer . apply_gradients ( zip ( gradients , trainable_vars ) ) # to make this push loss function agnostic average_loss = tf . reduce_mean ( loss_value ) self . constants . tracker . push_stats ( * stats ) return float ( average_loss )","title":"class QONetwork"},{"location":"quantum_oscillator/reference/network/#class-qonetworkkerasmodel","text":"","title":"class QONetwork(keras.Model)"},{"location":"quantum_oscillator/reference/network/#introduction","text":"This class contains actual implementation of neural network for solving quantum oscillator problem. Bases: keras . Model Source code in nneve/quantum_oscillator/network.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 class QONetwork ( keras . Model ): constants : QOConstants is_debug : bool is_console_mode : bool def __init__ ( self , constants : Optional [ QOConstants ] = None , is_debug : bool = False , is_console_mode : bool = True , name : str = \"QONetwork\" , ): \"\"\"Initialize quantum oscillator properties, neural network shape and create loss function object. Parameters ---------- constants : Optional[QOConstants], optional Constant values describing quantum oscillator, by default None is_debug : bool, optional debug mode switch, by default False is_console_mode : bool, optional When true, rich.Progress will be used to manifest learning process progress, by default True name : str, optional Aesthetic only, name for model, by default \"QONetwork\" \"\"\" self . constants = constants if constants is not None else QOConstants () self . is_console_mode = is_console_mode self . is_debug = is_debug # attribute access to cache function value self . loss_function # create network structure with two 50 neuron deep layers inputs , outputs = self . assemble_hook (( 50 , 50 )) # with constructor of keras.Model initialize model machinery super () . __init__ ( inputs = inputs , outputs = outputs , name = name , ) # not really needed thanks to custom train loop # but is called to satisfy internals of keras self . compile ( self . constants . optimizer , jit_compile = True , ) assert self . name is not None def assemble_hook ( self , deep_layers : Sequence [ int ] = ( 50 , 50 ) ) -> Tuple [ List [ keras . layers . InputLayer ], List [ keras . layers . Dense ]]: \"\"\"Construct neural network structure with specified number of deep layers used for transformation of input. Parameters ---------- deep_layers : Sequence[int], optional Number and size of deep layers, by default (50, 50) Returns ------- Tuple[List[keras.layers.InputLayer], List[keras.layers.Dense]] Input and output of neural network. \"\"\" # 1 value input layer inputs = cast ( keras . layers . InputLayer , keras . Input ( shape = ( 1 ,), name = \"input\" , dtype = tf . float32 , ), ) # One neuron decides what value should \u03bb have eigenvalue_out = Eigenvalue ( name = \"eigenvalue\" )( inputs ) input_and_eigenvalue = keras . layers . Concatenate ( axis = 1 , name = \"join\" , )([ inputs , eigenvalue_out ]) # dense layer seem not to be willing to accept two inputs # at the same time, therefore \u03bb and x are bound together # with help of another dense layer d1 = keras . layers . Dense ( 2 , activation = tf . sin , name = \"dense_input\" , dtype = tf . float32 , )( input_and_eigenvalue ) # dynamically defined number of deep layers, specified by # input argument `deep_layers`. They are joined sequentially deep_in = d1 deep_out = None for index , neuron_count in enumerate ( deep_layers ): deep_out = keras . layers . Dense ( neuron_count , activation = tf . sin , name = f \"dense_ { index } \" , dtype = tf . float32 , )( deep_in ) deep_in = deep_out del index # make sure no references by mistake # We require at least one deep layer to be available, otherwise # this network makes no sense assert deep_out is not None del deep_in # make sure no references by mistake # single value output from neural network outputs = keras . layers . Dense ( 1 , name = \"predictions\" , dtype = tf . float32 , )( deep_out ) # eigenvalue is not acquired by single layer call because it # was breaking learning process return [ inputs ], [ outputs , eigenvalue_out ] @cached_property def loss_function ( self ) -> LossFunctionT : # noqa: CFQ004, CFQ001 \"\"\"Create and return loss function used for learning process. Returns ------- LossFunctionT loss function object. \"\"\" @tf . function # type: ignore def loss_function_impl ( x : tf . Tensor , c : tf . Tensor , ) -> Tuple [ tf . Tensor , Tuple [ Any , ... ]]: # pragma: no cover # for more extensive description of loss function visit # https://argmaster.github.io/NNEVE/quantum_oscillator/introduction/ current_eigenvalue = self ( x )[ 1 ][ 0 ] deriv_x = tf . identity ( self . constants . get_sample ()) with tf . GradientTape () as second : second . watch ( deriv_x ) with tf . GradientTape () as first : first . watch ( deriv_x ) psi , _ = parametric_solution ( deriv_x ) # type: ignore dy_dx = first . gradient ( psi , deriv_x ) dy_dxx = second . gradient ( dy_dx , deriv_x ) residuum = tf . square ( tf . divide ( dy_dxx , - 2.0 * self . constants . mass ) + ( potential ( x ) * psi ) - ( current_eigenvalue * psi ) ) function_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( psi )), 1e-6 ), ) lambda_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( current_eigenvalue )), 1e-6 ) ) drive_loss = tf . exp ( tf . reduce_mean ( tf . subtract ( c , current_eigenvalue )) ) total_loss = residuum + function_loss + lambda_loss + drive_loss return total_loss , ( tf . reduce_mean ( total_loss ), current_eigenvalue , tf . reduce_mean ( residuum ), function_loss , lambda_loss , drive_loss , c , ) # type: ignore @tf . function # type: ignore def parametric_solution ( x : tf . Variable , ) -> Tuple [ tf . Tensor , tf . Tensor ]: # pragma: no cover psi , current_eigenvalue = self ( x ) return ( tf . add ( tf . constant ( self . constants . fb , dtype = tf . float32 ), tf . multiply ( boundary ( x ), psi ), ), current_eigenvalue [ 0 ], ) @tf . function # type: ignore def boundary ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return ( 1 - tf . exp ( tf . subtract ( self . constants . x_left , x ))) * ( 1 - tf . exp ( tf . subtract ( x , self . constants . x_right )) ) @tf . function # type: ignore def potential ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return tf . divide ( tf . multiply ( self . constants . k , tf . square ( x )), 2 ) if self . is_debug : self . debug_potential_function = potential self . debug_boundary_function = boundary self . debug_parametric_solution_function = parametric_solution self . debug_loss_function_function = loss_function_impl self . parametric_solution = parametric_solution return cast ( LossFunctionT , loss_function_impl ) def train_generations ( self , params : QOParams , generations : int = 5 , epochs : int = 1000 ) -> Iterable [ \"QONetwork\" ]: \"\"\"Train multiple generations of neural network. For each generation train() method is called, followed by update of QOParams. Parameters ---------- params : QOParams parameters describing quantum oscillator generations : int, optional number of generations to calculate, by default 5 epochs : int, optional number of epochs in each generation, by default 1000 Yields ------ Iterable[QONetwork] This neural network after generation is completed. \"\"\" with suppress ( KeyboardInterrupt ): for i in range ( generations ): logging . info ( f \"Generation: { i + 1 : 4.0f } our of \" f \" { generations : .0f } , ( { i / generations : .2% } )\" ) self . train ( params , epochs ) params . update () yield self def train ( # noqa: CCR001 self , params : QOParams , epochs : int = 10 ) -> None : \"\"\"Train single generation of neural network. Single generation consists of predefined number of epochs, each containing call to loss function and following improvement of neural network weights. Parameters ---------- params : QOParams parameters describing quantum oscillator epochs : int, optional number of epochs to calculate, by default 10 \"\"\" x = self . constants . get_sample () if self . is_console_mode : with Progress () as progress : task = progress . add_task ( description = \"Learning...\" , total = epochs + 1 ) for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) progress . update ( task , advance = 1 , description = description . capitalize (), ) else : for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) logging . info ( description ) def train_step ( self , x : tf . Tensor , params : QOParams ) -> float : \"\"\"Step of learning process. Step consists of single call to loss function and following improvement of network weights. Parameters ---------- x : tf.Tensor tensor containing X values grid. params : QOParams parameters describing quantum oscillator. Returns ------- float average loss of network before current improvement. \"\"\" with tf . GradientTape () as tape : loss_value , stats = self . loss_function ( x , * params . get_extra ()) trainable_vars = self . trainable_variables gradients = tape . gradient ( loss_value , trainable_vars ) self . constants . optimizer . apply_gradients ( zip ( gradients , trainable_vars ) ) # to make this push loss function agnostic average_loss = tf . reduce_mean ( loss_value ) self . constants . tracker . push_stats ( * stats ) return float ( average_loss ) def save ( self , filepath : Path ) -> None : # noqa: FNE003 \"\"\"Save model object to file. Parameters ---------- filepath : Path Path to file where model data should be saved. If file exists, it will be overwritten. \"\"\" weights = self . get_weights () with filepath . open ( \"wb\" ) as file : pickle . dump ( weights , file ) def load ( self , filepath : Path ) -> None : # noqa: FNE004 \"\"\"Load model data from file. Parameters ---------- filepath : Path Path to file where model data is stored. \"\"\" with filepath . open ( \"rb\" ) as file : weights = pickle . load ( file ) self . set_weights ( weights ) def plot_solution ( self ) -> None : \"\"\"Generate a plot with pyplot.plot which shows solution currently proposed by neural network.\"\"\" x = self . constants . get_sample () plt . plot ( x , self ( x )[ 0 ])","title":"Introduction"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.__init__","text":"Initialize quantum oscillator properties, neural network shape and create loss function object. Parameters: Name Type Description Default constants Optional [ QOConstants ], optional Constant values describing quantum oscillator, by default None None is_debug bool , optional debug mode switch, by default False False is_console_mode bool , optional When true, rich.Progress will be used to manifest learning process progress, by default True True name str , optional Aesthetic only, name for model, by default \"QONetwork\" 'QONetwork' Source code in nneve/quantum_oscillator/network.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , constants : Optional [ QOConstants ] = None , is_debug : bool = False , is_console_mode : bool = True , name : str = \"QONetwork\" , ): \"\"\"Initialize quantum oscillator properties, neural network shape and create loss function object. Parameters ---------- constants : Optional[QOConstants], optional Constant values describing quantum oscillator, by default None is_debug : bool, optional debug mode switch, by default False is_console_mode : bool, optional When true, rich.Progress will be used to manifest learning process progress, by default True name : str, optional Aesthetic only, name for model, by default \"QONetwork\" \"\"\" self . constants = constants if constants is not None else QOConstants () self . is_console_mode = is_console_mode self . is_debug = is_debug # attribute access to cache function value self . loss_function # create network structure with two 50 neuron deep layers inputs , outputs = self . assemble_hook (( 50 , 50 )) # with constructor of keras.Model initialize model machinery super () . __init__ ( inputs = inputs , outputs = outputs , name = name , ) # not really needed thanks to custom train loop # but is called to satisfy internals of keras self . compile ( self . constants . optimizer , jit_compile = True , ) assert self . name is not None","title":"__init__()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.assemble_hook","text":"Construct neural network structure with specified number of deep layers used for transformation of input. Parameters: Name Type Description Default deep_layers Sequence [ int ], optional Number and size of deep layers, by default (50, 50) (50, 50) Returns: Type Description Tuple [ List [ keras . layers . InputLayer ], List [ keras . layers . Dense ]] Input and output of neural network. Source code in nneve/quantum_oscillator/network.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def assemble_hook ( self , deep_layers : Sequence [ int ] = ( 50 , 50 ) ) -> Tuple [ List [ keras . layers . InputLayer ], List [ keras . layers . Dense ]]: \"\"\"Construct neural network structure with specified number of deep layers used for transformation of input. Parameters ---------- deep_layers : Sequence[int], optional Number and size of deep layers, by default (50, 50) Returns ------- Tuple[List[keras.layers.InputLayer], List[keras.layers.Dense]] Input and output of neural network. \"\"\" # 1 value input layer inputs = cast ( keras . layers . InputLayer , keras . Input ( shape = ( 1 ,), name = \"input\" , dtype = tf . float32 , ), ) # One neuron decides what value should \u03bb have eigenvalue_out = Eigenvalue ( name = \"eigenvalue\" )( inputs ) input_and_eigenvalue = keras . layers . Concatenate ( axis = 1 , name = \"join\" , )([ inputs , eigenvalue_out ]) # dense layer seem not to be willing to accept two inputs # at the same time, therefore \u03bb and x are bound together # with help of another dense layer d1 = keras . layers . Dense ( 2 , activation = tf . sin , name = \"dense_input\" , dtype = tf . float32 , )( input_and_eigenvalue ) # dynamically defined number of deep layers, specified by # input argument `deep_layers`. They are joined sequentially deep_in = d1 deep_out = None for index , neuron_count in enumerate ( deep_layers ): deep_out = keras . layers . Dense ( neuron_count , activation = tf . sin , name = f \"dense_ { index } \" , dtype = tf . float32 , )( deep_in ) deep_in = deep_out del index # make sure no references by mistake # We require at least one deep layer to be available, otherwise # this network makes no sense assert deep_out is not None del deep_in # make sure no references by mistake # single value output from neural network outputs = keras . layers . Dense ( 1 , name = \"predictions\" , dtype = tf . float32 , )( deep_out ) # eigenvalue is not acquired by single layer call because it # was breaking learning process return [ inputs ], [ outputs , eigenvalue_out ]","title":"assemble_hook()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.load","text":"Load model data from file. Parameters: Name Type Description Default filepath Path Path to file where model data is stored. required Source code in nneve/quantum_oscillator/network.py 359 360 361 362 363 364 365 366 367 368 369 def load ( self , filepath : Path ) -> None : # noqa: FNE004 \"\"\"Load model data from file. Parameters ---------- filepath : Path Path to file where model data is stored. \"\"\" with filepath . open ( \"rb\" ) as file : weights = pickle . load ( file ) self . set_weights ( weights )","title":"load()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.loss_function","text":"Create and return loss function used for learning process. Returns: Type Description LossFunctionT loss function object. Source code in nneve/quantum_oscillator/network.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 @cached_property def loss_function ( self ) -> LossFunctionT : # noqa: CFQ004, CFQ001 \"\"\"Create and return loss function used for learning process. Returns ------- LossFunctionT loss function object. \"\"\" @tf . function # type: ignore def loss_function_impl ( x : tf . Tensor , c : tf . Tensor , ) -> Tuple [ tf . Tensor , Tuple [ Any , ... ]]: # pragma: no cover # for more extensive description of loss function visit # https://argmaster.github.io/NNEVE/quantum_oscillator/introduction/ current_eigenvalue = self ( x )[ 1 ][ 0 ] deriv_x = tf . identity ( self . constants . get_sample ()) with tf . GradientTape () as second : second . watch ( deriv_x ) with tf . GradientTape () as first : first . watch ( deriv_x ) psi , _ = parametric_solution ( deriv_x ) # type: ignore dy_dx = first . gradient ( psi , deriv_x ) dy_dxx = second . gradient ( dy_dx , deriv_x ) residuum = tf . square ( tf . divide ( dy_dxx , - 2.0 * self . constants . mass ) + ( potential ( x ) * psi ) - ( current_eigenvalue * psi ) ) function_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( psi )), 1e-6 ), ) lambda_loss = tf . divide ( 1 , tf . add ( tf . reduce_mean ( tf . square ( current_eigenvalue )), 1e-6 ) ) drive_loss = tf . exp ( tf . reduce_mean ( tf . subtract ( c , current_eigenvalue )) ) total_loss = residuum + function_loss + lambda_loss + drive_loss return total_loss , ( tf . reduce_mean ( total_loss ), current_eigenvalue , tf . reduce_mean ( residuum ), function_loss , lambda_loss , drive_loss , c , ) # type: ignore @tf . function # type: ignore def parametric_solution ( x : tf . Variable , ) -> Tuple [ tf . Tensor , tf . Tensor ]: # pragma: no cover psi , current_eigenvalue = self ( x ) return ( tf . add ( tf . constant ( self . constants . fb , dtype = tf . float32 ), tf . multiply ( boundary ( x ), psi ), ), current_eigenvalue [ 0 ], ) @tf . function # type: ignore def boundary ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return ( 1 - tf . exp ( tf . subtract ( self . constants . x_left , x ))) * ( 1 - tf . exp ( tf . subtract ( x , self . constants . x_right )) ) @tf . function # type: ignore def potential ( x : tf . Tensor ) -> tf . Tensor : # pragma: no cover return tf . divide ( tf . multiply ( self . constants . k , tf . square ( x )), 2 ) if self . is_debug : self . debug_potential_function = potential self . debug_boundary_function = boundary self . debug_parametric_solution_function = parametric_solution self . debug_loss_function_function = loss_function_impl self . parametric_solution = parametric_solution return cast ( LossFunctionT , loss_function_impl )","title":"loss_function()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.plot_solution","text":"Generate a plot with pyplot.plot which shows solution currently proposed by neural network. Source code in nneve/quantum_oscillator/network.py 371 372 373 374 375 def plot_solution ( self ) -> None : \"\"\"Generate a plot with pyplot.plot which shows solution currently proposed by neural network.\"\"\" x = self . constants . get_sample () plt . plot ( x , self ( x )[ 0 ])","title":"plot_solution()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.save","text":"Save model object to file. Parameters: Name Type Description Default filepath Path Path to file where model data should be saved. If file exists, it will be overwritten. required Source code in nneve/quantum_oscillator/network.py 346 347 348 349 350 351 352 353 354 355 356 357 def save ( self , filepath : Path ) -> None : # noqa: FNE003 \"\"\"Save model object to file. Parameters ---------- filepath : Path Path to file where model data should be saved. If file exists, it will be overwritten. \"\"\" weights = self . get_weights () with filepath . open ( \"wb\" ) as file : pickle . dump ( weights , file )","title":"save()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.train","text":"Train single generation of neural network. Single generation consists of predefined number of epochs, each containing call to loss function and following improvement of neural network weights. Parameters: Name Type Description Default params QOParams parameters describing quantum oscillator required epochs int , optional number of epochs to calculate, by default 10 10 Source code in nneve/quantum_oscillator/network.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def train ( # noqa: CCR001 self , params : QOParams , epochs : int = 10 ) -> None : \"\"\"Train single generation of neural network. Single generation consists of predefined number of epochs, each containing call to loss function and following improvement of neural network weights. Parameters ---------- params : QOParams parameters describing quantum oscillator epochs : int, optional number of epochs to calculate, by default 10 \"\"\" x = self . constants . get_sample () if self . is_console_mode : with Progress () as progress : task = progress . add_task ( description = \"Learning...\" , total = epochs + 1 ) for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) progress . update ( task , advance = 1 , description = description . capitalize (), ) else : for i in range ( epochs ): self . train_step ( x , params ) description = self . constants . tracker . get_trace ( i ) logging . info ( description )","title":"train()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.train_generations","text":"Train multiple generations of neural network. For each generation train() method is called, followed by update of QOParams. Parameters: Name Type Description Default params QOParams parameters describing quantum oscillator required generations int , optional number of generations to calculate, by default 5 5 epochs int , optional number of epochs in each generation, by default 1000 1000 Yields: Type Description Iterable [ QONetwork ] This neural network after generation is completed. Source code in nneve/quantum_oscillator/network.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def train_generations ( self , params : QOParams , generations : int = 5 , epochs : int = 1000 ) -> Iterable [ \"QONetwork\" ]: \"\"\"Train multiple generations of neural network. For each generation train() method is called, followed by update of QOParams. Parameters ---------- params : QOParams parameters describing quantum oscillator generations : int, optional number of generations to calculate, by default 5 epochs : int, optional number of epochs in each generation, by default 1000 Yields ------ Iterable[QONetwork] This neural network after generation is completed. \"\"\" with suppress ( KeyboardInterrupt ): for i in range ( generations ): logging . info ( f \"Generation: { i + 1 : 4.0f } our of \" f \" { generations : .0f } , ( { i / generations : .2% } )\" ) self . train ( params , epochs ) params . update () yield self","title":"train_generations()"},{"location":"quantum_oscillator/reference/network/#nneve.quantum_oscillator.network.QONetwork.train_step","text":"Step of learning process. Step consists of single call to loss function and following improvement of network weights. Parameters: Name Type Description Default x tf . Tensor tensor containing X values grid. required params QOParams parameters describing quantum oscillator. required Returns: Type Description float average loss of network before current improvement. Source code in nneve/quantum_oscillator/network.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def train_step ( self , x : tf . Tensor , params : QOParams ) -> float : \"\"\"Step of learning process. Step consists of single call to loss function and following improvement of network weights. Parameters ---------- x : tf.Tensor tensor containing X values grid. params : QOParams parameters describing quantum oscillator. Returns ------- float average loss of network before current improvement. \"\"\" with tf . GradientTape () as tape : loss_value , stats = self . loss_function ( x , * params . get_extra ()) trainable_vars = self . trainable_variables gradients = tape . gradient ( loss_value , trainable_vars ) self . constants . optimizer . apply_gradients ( zip ( gradients , trainable_vars ) ) # to make this push loss function agnostic average_loss = tf . reduce_mean ( loss_value ) self . constants . tracker . push_stats ( * stats ) return float ( average_loss )","title":"train_step()"},{"location":"quantum_oscillator/reference/params/","text":"class QOParams ( pydantic . BaseModel ) # Parent classes # class pydantic . BaseModel Introduction # class QOParams is responsible for holding variables which can change during the learning of class QONetwork . This class can (should) also change the values of the variables in using a dedicated method that is called after each generation of the learning process. Instance attributes # Note Attributes are mutable Arbitrary types are allowed to be used as attribute values c : float # While searching for eigenvalues, learning algorithm increases value of c by 0.16 every genration, which increases Ldrive. That forces the network to search for larger eigenvalues and the associated eigenfunctions_ Instance methods # def update ( self ) -> None # Called after each network learning generation to (optionally) change the values of the variables stored in the attributes of the QOParams class def extra ( self ) -> Tuple [ float , ... ] # Returns a tuple containing the variables to be passed to the cost function. The order of the variables is arbitrary, but note that the tuple is unpacked in the function call. This method was created to overcome the problem of passing class instances to tensorflow graph functions.","title":"class QOParams"},{"location":"quantum_oscillator/reference/params/#class-qoparamspydanticbasemodel","text":"","title":"class QOParams(pydantic.BaseModel)"},{"location":"quantum_oscillator/reference/params/#parent-classes","text":"class pydantic . BaseModel","title":"Parent classes"},{"location":"quantum_oscillator/reference/params/#introduction","text":"class QOParams is responsible for holding variables which can change during the learning of class QONetwork . This class can (should) also change the values of the variables in using a dedicated method that is called after each generation of the learning process.","title":"Introduction"},{"location":"quantum_oscillator/reference/params/#instance-attributes","text":"Note Attributes are mutable Arbitrary types are allowed to be used as attribute values","title":"Instance attributes"},{"location":"quantum_oscillator/reference/params/#c-float","text":"While searching for eigenvalues, learning algorithm increases value of c by 0.16 every genration, which increases Ldrive. That forces the network to search for larger eigenvalues and the associated eigenfunctions_","title":"c: float"},{"location":"quantum_oscillator/reference/params/#instance-methods","text":"","title":"Instance methods"},{"location":"quantum_oscillator/reference/params/#def-updateself-none","text":"Called after each network learning generation to (optionally) change the values of the variables stored in the attributes of the QOParams class","title":"def update(self) -&gt; None"},{"location":"quantum_oscillator/reference/params/#def-extraself-tuplefloat","text":"Returns a tuple containing the variables to be passed to the cost function. The order of the variables is arbitrary, but note that the tuple is unpacked in the function call. This method was created to overcome the problem of passing class instances to tensorflow graph functions.","title":"def extra(self) -&gt; Tuple[float, ...]"},{"location":"quantum_oscillator/reference/tracker/","text":"class QOTracker ( pydantic . BaseModel ) # Parent classes # class pydantic . BaseModel Note class QOTracker is an element of internal API of nneve . quantum_oscillator subpackage was only ment to interact with class QONetwork and class QOConstants . class QOTracker is responsible for gathering network state metrics during network learning process. The class is also equipped with methods to plot graphs based on the collected data. The class is also equipped with methods to plot graphs based on the collected data. Its additional ability is to create network status messages that are shown on the command line while the network is being learned. Instance attributes # Note Attributes are mutable Arbitrary types are allowed to be used as attribute values c : float # total_loss : List [ float ] = Field ( default_factory = list ) # Stores history of total_loss values which were presented by the network during training process. eigenvalue : List [ float ] = Field ( default_factory = list ) # Stores history of eigenvalue values which were presented by the network during training process. residuum : List [ float ] = Field ( default_factory = list ) # Stores history of residuum values which were presented by the network during training process. function_loss : List [ float ] = Field ( default_factory = list ) # Stores history of function_loss values which were presented by the network during training process. eigenvalue_loss : List [ float ] = Field ( default_factory = list ) # Stores history of eigenvalue_loss values which were presented by the network during training process. drive_loss : List [ float ] = Field ( default_factory = list ) # Stores history of drive_loss values which were presented by the network during training process. c : List [ float ] = Field ( default_factory = list ) # Stores history of c values which were presented by the network during training process. Instance methods # def push_stats ( self , ... ) -> None # Parameters # Appends passed positional arguments to corresponding histories. name type description total_loss float value to be appended to total_loss history list. eigenvalue float value to be appended to eigenvalue history list. residuum float value to be appended to residuum history list. function_loss float value to be appended to function_loss history list. eigenvalue_loss float value to be appended to eigenvalue_loss history list. drive_loss float value to be appended to drive_loss history list. c float value to be appended to c history list. *_ Any extra arguments are ignored. Returns: # type description NoneType no value is returned","title":"class QOTracker"},{"location":"quantum_oscillator/reference/tracker/#class-qotrackerpydanticbasemodel","text":"","title":"class QOTracker(pydantic.BaseModel)"},{"location":"quantum_oscillator/reference/tracker/#parent-classes","text":"class pydantic . BaseModel Note class QOTracker is an element of internal API of nneve . quantum_oscillator subpackage was only ment to interact with class QONetwork and class QOConstants . class QOTracker is responsible for gathering network state metrics during network learning process. The class is also equipped with methods to plot graphs based on the collected data. The class is also equipped with methods to plot graphs based on the collected data. Its additional ability is to create network status messages that are shown on the command line while the network is being learned.","title":"Parent classes"},{"location":"quantum_oscillator/reference/tracker/#instance-attributes","text":"Note Attributes are mutable Arbitrary types are allowed to be used as attribute values","title":"Instance attributes"},{"location":"quantum_oscillator/reference/tracker/#c-float","text":"","title":"c: float"},{"location":"quantum_oscillator/reference/tracker/#total_loss-listfloat-fielddefault_factorylist","text":"Stores history of total_loss values which were presented by the network during training process.","title":"total_loss: List[float] = Field(default_factory=list)"},{"location":"quantum_oscillator/reference/tracker/#eigenvalue-listfloat-fielddefault_factorylist","text":"Stores history of eigenvalue values which were presented by the network during training process.","title":"eigenvalue: List[float] = Field(default_factory=list)"},{"location":"quantum_oscillator/reference/tracker/#residuum-listfloat-fielddefault_factorylist","text":"Stores history of residuum values which were presented by the network during training process.","title":"residuum: List[float] = Field(default_factory=list)"},{"location":"quantum_oscillator/reference/tracker/#function_loss-listfloat-fielddefault_factorylist","text":"Stores history of function_loss values which were presented by the network during training process.","title":"function_loss: List[float] = Field(default_factory=list)"},{"location":"quantum_oscillator/reference/tracker/#eigenvalue_loss-listfloat-fielddefault_factorylist","text":"Stores history of eigenvalue_loss values which were presented by the network during training process.","title":"eigenvalue_loss: List[float] = Field(default_factory=list)"},{"location":"quantum_oscillator/reference/tracker/#drive_loss-listfloat-fielddefault_factorylist","text":"Stores history of drive_loss values which were presented by the network during training process.","title":"drive_loss: List[float] = Field(default_factory=list)"},{"location":"quantum_oscillator/reference/tracker/#c-listfloat-fielddefault_factorylist","text":"Stores history of c values which were presented by the network during training process.","title":"c: List[float] = Field(default_factory=list)"},{"location":"quantum_oscillator/reference/tracker/#instance-methods","text":"","title":"Instance methods"},{"location":"quantum_oscillator/reference/tracker/#def-push_statsself-none","text":"","title":"def push_stats(self, ...) -&gt; None"},{"location":"quantum_oscillator/reference/tracker/#parameters","text":"Appends passed positional arguments to corresponding histories. name type description total_loss float value to be appended to total_loss history list. eigenvalue float value to be appended to eigenvalue history list. residuum float value to be appended to residuum history list. function_loss float value to be appended to function_loss history list. eigenvalue_loss float value to be appended to eigenvalue_loss history list. drive_loss float value to be appended to drive_loss history list. c float value to be appended to c history list. *_ Any extra arguments are ignored.","title":"Parameters"},{"location":"quantum_oscillator/reference/tracker/#returns","text":"type description NoneType no value is returned","title":"Returns:"}]}